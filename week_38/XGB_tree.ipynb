{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d34bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/oliver/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import Preprocessor as p\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch.nn.functional as  F\n",
    "import csv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helpMe import get_ppl, convert_to_float, getList,train_rf, train_loop, Average, map_boolean, generate_embeddings, write_embeddings_to_csv\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from customDataset import CustomDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from modelNN import SimpleNN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbcdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b50f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the copenlu/answerable_tydiqa dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "train_df = train_df[train_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "train_df = p.DataFramePreprocessor(train_df).df\n",
    "\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "val_df = val_df[val_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "val_df = p.DataFramePreprocessor(val_df).df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a2f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1a33988",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df\n",
    "data['document_length'] = data['document_plaintext'].str.len()\n",
    "data['is_answerable'] = data['annotations'].apply(lambda x: x.get('answer_start', [-1])[0] != -1)\n",
    "\n",
    "val_df['is_answerable'] = data['annotations'].apply(lambda x: x.get('answer_start', [-1])[0] != -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26e7d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_answerable'] = data['is_answerable'].apply(map_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e33da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali = data[data['language'] == 'bengali']\n",
    "arabic = data[data['language'] == 'arabic']\n",
    "indonesian = data[data['language'] == 'indonesian']\n",
    "\n",
    "\n",
    "bengali_val = val_df[val_df['language'] == 'bengali']\n",
    "arabic_val = val_df[val_df['language'] == 'arabic']\n",
    "indonesian_val = val_df[val_df['language'] == 'indonesian']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e4442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcc0667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arab_embeddings = pd.read_csv('embeddings_arabic.csv',header=None)\n",
    "arab_embeddings = arab_embeddings[1]\n",
    "\n",
    "bengali_embeddings = pd.read_csv('embeddings_bengali.csv',header=None)\n",
    "bengali_embeddings = bengali_embeddings[1]\n",
    "\n",
    "indo_embeddings = pd.read_csv('embeddings.csv',header=None)\n",
    "indo_embeddings = indo_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5e046b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numpy_array(s):\n",
    "    numeric_values = re.findall(r'-?\\d+\\.\\d+', s)\n",
    "    numpy_array = np.array(numeric_values, dtype=float)\n",
    "    return numpy_array\n",
    "\n",
    "\n",
    "bengali_array = bengali_embeddings.apply(convert_to_numpy_array)\n",
    "arabic_array =  arab_embeddings.apply(convert_to_numpy_array)\n",
    "indonesian_array =  indo_embeddings.apply(convert_to_numpy_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd709065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7f304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4f6ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "benagali_y = np.array(bengali['is_answerable'])\n",
    "arabic_y = np.array(arabic['is_answerable'])\n",
    "indonesian_y = np.array(indonesian['is_answerable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ccd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def measure_empirical_randomness(arr):\n",
    "    \n",
    "    length = len(arr)\n",
    "    random_sequence = np.random.randint(2, size=length)\n",
    "    matching_elements = np.sum(arr == random_sequence)\n",
    "    empirical_randomness = matching_elements / length\n",
    "    \n",
    "    return empirical_randomness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121dca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "beng_random = measure_empirical_randomness(benagali_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07bb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b02b3fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(bengali_array[0]))\n",
    "print(type(benagali_y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be62cdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a76f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_data, targets):\n",
    "        self.input_data = torch.FloatTensor(input_data)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_sample = self.input_data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return input_sample, target\n",
    "    \n",
    "    def split_dataset(self, split_ratio=0.8, random_seed=None):\n",
    "\n",
    "        # Calculate the sizes of training and validation sets based on split_ratio\n",
    "        train_size = int(split_ratio * len(self))\n",
    "        val_size = len(self) - train_size\n",
    "        \n",
    "        # Use random_split to split the dataset into training and validation subsets\n",
    "        train_dataset, val_dataset = random_split(self, [train_size, val_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "        \n",
    "        return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a00b5513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14386/2824331789.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  self.input_data = torch.FloatTensor(input_data)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  \n",
    "\n",
    "bengali_data = CustomDataset(bengali_array, benagali_y)\n",
    "bengali_train, bengali_test = bengali_data.split_dataset(split_ratio=0.8, random_seed=42)\n",
    "train_loader_bengali = DataLoader(bengali_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader_bengali = DataLoader(bengali_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "arab_data = CustomDataset(arabic_array, arabic_y)\n",
    "arab_train, arab_test = arab_data.split_dataset(split_ratio=0.8, random_seed=42)\n",
    "train_loader_arab = DataLoader(arab_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader_arab = DataLoader(arab_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "indonesian_data = CustomDataset(indonesian_array, indonesian_y)\n",
    "indonesian_train, indonesian_test = indonesian_data.split_dataset(split_ratio=0.8, random_seed=42)\n",
    "train_loader_indonesian = DataLoader(indonesian_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader_indonesian = DataLoader(indonesian_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ccaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b256e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(768,128,256,128, 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ed0a6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Loss: 0.6373\n",
      "Epoch [2/400], Loss: 0.5681\n",
      "Epoch [3/400], Loss: 0.5574\n",
      "Epoch [4/400], Loss: 0.5470\n",
      "Epoch [5/400], Loss: 0.5400\n",
      "Epoch [6/400], Loss: 0.5297\n",
      "Epoch [7/400], Loss: 0.5202\n",
      "Epoch [8/400], Loss: 0.5241\n",
      "Epoch [9/400], Loss: 0.5093\n",
      "Epoch [10/400], Loss: 0.5105\n",
      "Epoch [11/400], Loss: 0.5039\n",
      "Epoch [12/400], Loss: 0.4989\n",
      "Epoch [13/400], Loss: 0.4922\n",
      "Epoch [14/400], Loss: 0.4864\n",
      "Epoch [15/400], Loss: 0.4779\n",
      "Epoch [16/400], Loss: 0.4793\n",
      "Epoch [17/400], Loss: 0.4807\n",
      "Epoch [18/400], Loss: 0.4670\n",
      "Epoch [19/400], Loss: 0.4753\n",
      "Epoch [20/400], Loss: 0.4503\n",
      "Epoch [21/400], Loss: 0.4500\n",
      "Epoch [22/400], Loss: 0.4551\n",
      "Epoch [23/400], Loss: 0.4320\n",
      "Epoch [24/400], Loss: 0.4357\n",
      "Epoch [25/400], Loss: 0.4469\n",
      "Epoch [26/400], Loss: 0.4478\n",
      "Epoch [27/400], Loss: 0.4256\n",
      "Epoch [28/400], Loss: 0.4276\n",
      "Epoch [29/400], Loss: 0.4145\n",
      "Epoch [30/400], Loss: 0.4305\n",
      "Epoch [31/400], Loss: 0.4272\n",
      "Epoch [32/400], Loss: 0.4003\n",
      "Epoch [33/400], Loss: 0.4113\n",
      "Epoch [34/400], Loss: 0.4272\n",
      "Epoch [35/400], Loss: 0.3941\n",
      "Epoch [36/400], Loss: 0.3997\n",
      "Epoch [37/400], Loss: 0.3794\n",
      "Epoch [38/400], Loss: 0.3904\n",
      "Epoch [39/400], Loss: 0.3858\n",
      "Epoch [40/400], Loss: 0.3748\n",
      "Epoch [41/400], Loss: 0.3982\n",
      "Epoch [42/400], Loss: 0.3659\n",
      "Epoch [43/400], Loss: 0.3942\n",
      "Epoch [44/400], Loss: 0.3539\n",
      "Epoch [45/400], Loss: 0.3512\n",
      "Epoch [46/400], Loss: 0.3693\n",
      "Epoch [47/400], Loss: 0.3344\n",
      "Epoch [48/400], Loss: 0.3406\n",
      "Epoch [49/400], Loss: 0.3291\n",
      "Epoch [50/400], Loss: 0.3490\n",
      "Epoch [51/400], Loss: 0.3715\n",
      "Epoch [52/400], Loss: 0.3802\n",
      "Epoch [53/400], Loss: 0.3888\n",
      "Epoch [54/400], Loss: 0.3443\n",
      "Epoch [55/400], Loss: 0.3910\n",
      "Epoch [56/400], Loss: 0.3319\n",
      "Epoch [57/400], Loss: 0.3273\n",
      "Epoch [58/400], Loss: 0.3252\n",
      "Epoch [59/400], Loss: 0.3396\n",
      "Epoch [60/400], Loss: 0.3505\n",
      "Epoch [61/400], Loss: 0.3168\n",
      "Epoch [62/400], Loss: 0.3201\n",
      "Epoch [63/400], Loss: 0.3171\n",
      "Epoch [64/400], Loss: 0.3282\n",
      "Epoch [65/400], Loss: 0.2982\n",
      "Epoch [66/400], Loss: 0.3224\n",
      "Epoch [67/400], Loss: 0.3216\n",
      "Epoch [68/400], Loss: 0.3018\n",
      "Epoch [69/400], Loss: 0.3319\n",
      "Epoch [70/400], Loss: 0.3068\n",
      "Epoch [71/400], Loss: 0.3184\n",
      "Epoch [72/400], Loss: 0.3031\n",
      "Epoch [73/400], Loss: 0.3063\n",
      "Epoch [74/400], Loss: 0.2839\n",
      "Epoch [75/400], Loss: 0.3372\n",
      "Epoch [76/400], Loss: 0.2848\n",
      "Epoch [77/400], Loss: 0.3139\n",
      "Epoch [78/400], Loss: 0.3086\n",
      "Epoch [79/400], Loss: 0.3091\n",
      "Epoch [80/400], Loss: 0.2777\n",
      "Epoch [81/400], Loss: 0.2886\n",
      "Epoch [82/400], Loss: 0.3072\n",
      "Epoch [83/400], Loss: 0.2690\n",
      "Epoch [84/400], Loss: 0.2887\n",
      "Epoch [85/400], Loss: 0.3170\n",
      "Epoch [86/400], Loss: 0.2587\n",
      "Epoch [87/400], Loss: 0.2561\n",
      "Epoch [88/400], Loss: 0.2993\n",
      "Epoch [89/400], Loss: 0.2634\n",
      "Epoch [90/400], Loss: 0.2864\n",
      "Epoch [91/400], Loss: 0.2437\n",
      "Epoch [92/400], Loss: 0.2792\n",
      "Epoch [93/400], Loss: 0.2700\n",
      "Epoch [94/400], Loss: 0.2598\n",
      "Epoch [95/400], Loss: 0.2548\n",
      "Epoch [96/400], Loss: 0.2846\n",
      "Epoch [97/400], Loss: 0.2448\n",
      "Epoch [98/400], Loss: 0.2921\n",
      "Epoch [99/400], Loss: 0.2478\n",
      "Epoch [100/400], Loss: 0.2469\n",
      "Epoch [101/400], Loss: 0.2876\n",
      "Epoch [102/400], Loss: 0.2511\n",
      "Epoch [103/400], Loss: 0.2563\n",
      "Epoch [104/400], Loss: 0.2758\n",
      "Epoch [105/400], Loss: 0.2307\n",
      "Epoch [106/400], Loss: 0.3746\n",
      "Epoch [107/400], Loss: 0.2622\n",
      "Epoch [108/400], Loss: 0.2308\n",
      "Epoch [109/400], Loss: 0.2642\n",
      "Epoch [110/400], Loss: 0.2774\n",
      "Epoch [111/400], Loss: 0.2296\n",
      "Epoch [112/400], Loss: 0.2802\n",
      "Epoch [113/400], Loss: 0.3092\n",
      "Epoch [114/400], Loss: 0.2646\n",
      "Epoch [115/400], Loss: 0.2505\n",
      "Epoch [116/400], Loss: 0.2407\n",
      "Epoch [117/400], Loss: 0.2904\n",
      "Epoch [118/400], Loss: 0.2822\n",
      "Epoch [119/400], Loss: 0.2521\n",
      "Epoch [120/400], Loss: 0.2729\n",
      "Epoch [121/400], Loss: 0.2428\n",
      "Epoch [122/400], Loss: 0.2193\n",
      "Epoch [123/400], Loss: 0.2088\n",
      "Epoch [124/400], Loss: 0.2206\n",
      "Epoch [125/400], Loss: 0.3608\n",
      "Epoch [126/400], Loss: 0.3862\n",
      "Epoch [127/400], Loss: 0.3500\n",
      "Epoch [128/400], Loss: 0.3425\n",
      "Epoch [129/400], Loss: 0.3431\n",
      "Epoch [130/400], Loss: 0.3415\n",
      "Epoch [131/400], Loss: 0.3616\n",
      "Epoch [132/400], Loss: 0.3289\n",
      "Epoch [133/400], Loss: 0.3641\n",
      "Epoch [134/400], Loss: 0.3470\n",
      "Epoch [135/400], Loss: 0.3417\n",
      "Epoch [136/400], Loss: 0.3242\n",
      "Epoch [137/400], Loss: 0.3802\n",
      "Epoch [138/400], Loss: 0.3279\n",
      "Epoch [139/400], Loss: 0.3288\n",
      "Epoch [140/400], Loss: 0.3255\n",
      "Epoch [141/400], Loss: 0.3717\n",
      "Epoch [142/400], Loss: 0.3472\n",
      "Epoch [143/400], Loss: 0.3287\n",
      "Epoch [144/400], Loss: 0.3495\n",
      "Epoch [145/400], Loss: 0.3142\n",
      "Epoch [146/400], Loss: 0.3292\n",
      "Epoch [147/400], Loss: 0.3111\n",
      "Epoch [148/400], Loss: 0.3204\n",
      "Epoch [149/400], Loss: 0.3273\n",
      "Epoch [150/400], Loss: 0.3319\n",
      "Epoch [151/400], Loss: 0.3254\n",
      "Epoch [152/400], Loss: 0.3566\n",
      "Epoch [153/400], Loss: 0.3244\n",
      "Epoch [154/400], Loss: 0.3248\n",
      "Epoch [155/400], Loss: 0.3171\n",
      "Epoch [156/400], Loss: 0.3367\n",
      "Epoch [157/400], Loss: 0.3296\n",
      "Epoch [158/400], Loss: 0.3229\n",
      "Epoch [159/400], Loss: 0.3065\n",
      "Epoch [160/400], Loss: 0.3112\n",
      "Epoch [161/400], Loss: 0.4198\n",
      "Epoch [162/400], Loss: 0.3247\n",
      "Epoch [163/400], Loss: 0.2999\n",
      "Epoch [164/400], Loss: 0.3175\n",
      "Epoch [165/400], Loss: 0.3084\n",
      "Epoch [166/400], Loss: 0.3036\n",
      "Epoch [167/400], Loss: 0.3031\n",
      "Epoch [168/400], Loss: 0.3079\n",
      "Epoch [169/400], Loss: 0.3297\n",
      "Epoch [170/400], Loss: 0.2889\n",
      "Epoch [171/400], Loss: 0.3334\n",
      "Epoch [172/400], Loss: 0.2916\n",
      "Epoch [173/400], Loss: 0.3041\n",
      "Epoch [174/400], Loss: 0.3024\n",
      "Epoch [175/400], Loss: 0.3097\n",
      "Epoch [176/400], Loss: 0.2892\n",
      "Epoch [177/400], Loss: 0.2974\n",
      "Epoch [178/400], Loss: 0.3266\n",
      "Epoch [179/400], Loss: 0.2859\n",
      "Epoch [180/400], Loss: 0.2960\n",
      "Epoch [181/400], Loss: 0.2959\n",
      "Epoch [182/400], Loss: 0.2939\n",
      "Epoch [183/400], Loss: 0.3137\n",
      "Epoch [184/400], Loss: 0.2813\n",
      "Epoch [185/400], Loss: 0.2889\n",
      "Epoch [186/400], Loss: 0.2902\n",
      "Epoch [187/400], Loss: 0.2948\n",
      "Epoch [188/400], Loss: 0.3264\n",
      "Epoch [189/400], Loss: 0.2935\n",
      "Epoch [190/400], Loss: 0.2804\n",
      "Epoch [191/400], Loss: 0.2824\n",
      "Epoch [192/400], Loss: 0.2751\n",
      "Epoch [193/400], Loss: 0.2652\n",
      "Epoch [194/400], Loss: 0.2804\n",
      "Epoch [195/400], Loss: 0.4399\n",
      "Epoch [196/400], Loss: 0.4412\n",
      "Epoch [197/400], Loss: 0.4255\n",
      "Epoch [198/400], Loss: 0.4290\n",
      "Epoch [199/400], Loss: 0.4159\n",
      "Epoch [200/400], Loss: 0.4093\n",
      "Epoch [201/400], Loss: 0.4238\n",
      "Epoch [202/400], Loss: 0.4202\n",
      "Epoch [203/400], Loss: 0.4163\n",
      "Epoch [204/400], Loss: 0.4100\n",
      "Epoch [205/400], Loss: 0.3658\n",
      "Epoch [206/400], Loss: 0.3188\n",
      "Epoch [207/400], Loss: 0.3210\n",
      "Epoch [208/400], Loss: 0.3175\n",
      "Epoch [209/400], Loss: 0.2721\n",
      "Epoch [210/400], Loss: 0.2990\n",
      "Epoch [211/400], Loss: 0.3154\n",
      "Epoch [212/400], Loss: 0.2974\n",
      "Epoch [213/400], Loss: 0.2729\n",
      "Epoch [214/400], Loss: 0.3379\n",
      "Epoch [215/400], Loss: 0.3435\n",
      "Epoch [216/400], Loss: 0.2816\n",
      "Epoch [217/400], Loss: 0.2617\n",
      "Epoch [218/400], Loss: 0.2694\n",
      "Epoch [219/400], Loss: 0.2808\n",
      "Epoch [220/400], Loss: 0.2624\n",
      "Epoch [221/400], Loss: 0.2808\n",
      "Epoch [222/400], Loss: 0.2784\n",
      "Epoch [223/400], Loss: 0.3416\n",
      "Epoch [224/400], Loss: 0.2904\n",
      "Epoch [225/400], Loss: 0.2670\n",
      "Epoch [226/400], Loss: 0.2584\n",
      "Epoch [227/400], Loss: 0.2519\n",
      "Epoch [228/400], Loss: 0.2862\n",
      "Epoch [229/400], Loss: 0.2464\n",
      "Epoch [230/400], Loss: 0.2561\n",
      "Epoch [231/400], Loss: 0.2604\n",
      "Epoch [232/400], Loss: 0.3209\n",
      "Epoch [233/400], Loss: 0.2511\n",
      "Epoch [234/400], Loss: 0.2399\n",
      "Epoch [235/400], Loss: 0.2642\n",
      "Epoch [236/400], Loss: 0.2529\n",
      "Epoch [237/400], Loss: 0.3288\n",
      "Epoch [238/400], Loss: 0.3419\n",
      "Epoch [239/400], Loss: 0.2758\n",
      "Epoch [240/400], Loss: 0.2424\n",
      "Epoch [241/400], Loss: 0.2559\n",
      "Epoch [242/400], Loss: 0.2285\n",
      "Epoch [243/400], Loss: 0.2340\n",
      "Epoch [244/400], Loss: 0.2158\n",
      "Epoch [245/400], Loss: 0.3048\n",
      "Epoch [246/400], Loss: 0.2514\n",
      "Epoch [247/400], Loss: 0.2313\n",
      "Epoch [248/400], Loss: 0.2494\n",
      "Epoch [249/400], Loss: 0.2519\n",
      "Epoch [250/400], Loss: 0.2219\n",
      "Epoch [251/400], Loss: 0.2269\n",
      "Epoch [252/400], Loss: 0.2430\n",
      "Epoch [253/400], Loss: 0.2564\n",
      "Epoch [254/400], Loss: 0.2774\n",
      "Epoch [255/400], Loss: 0.2330\n",
      "Epoch [256/400], Loss: 0.2514\n",
      "Epoch [257/400], Loss: 0.2178\n",
      "Epoch [258/400], Loss: 0.2209\n",
      "Epoch [259/400], Loss: 0.3217\n",
      "Epoch [260/400], Loss: 0.2415\n",
      "Epoch [261/400], Loss: 0.2161\n",
      "Epoch [262/400], Loss: 0.2435\n",
      "Epoch [263/400], Loss: 0.2204\n",
      "Epoch [264/400], Loss: 0.2334\n",
      "Epoch [265/400], Loss: 0.2573\n",
      "Epoch [266/400], Loss: 0.2630\n",
      "Epoch [267/400], Loss: 0.2468\n",
      "Epoch [268/400], Loss: 0.2242\n",
      "Epoch [269/400], Loss: 0.2546\n",
      "Epoch [270/400], Loss: 0.2308\n",
      "Epoch [271/400], Loss: 0.2425\n",
      "Epoch [272/400], Loss: 0.2266\n",
      "Epoch [273/400], Loss: 0.2235\n",
      "Epoch [274/400], Loss: 0.2459\n",
      "Epoch [275/400], Loss: 0.3551\n",
      "Epoch [276/400], Loss: 0.2620\n",
      "Epoch [277/400], Loss: 0.2363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [278/400], Loss: 0.2603\n",
      "Epoch [279/400], Loss: 0.2411\n",
      "Epoch [280/400], Loss: 0.2412\n",
      "Epoch [281/400], Loss: 0.2035\n",
      "Epoch [282/400], Loss: 0.2701\n",
      "Epoch [283/400], Loss: 0.2893\n",
      "Epoch [284/400], Loss: 0.2212\n",
      "Epoch [285/400], Loss: 0.2194\n",
      "Epoch [286/400], Loss: 0.2497\n",
      "Epoch [287/400], Loss: 0.2274\n",
      "Epoch [288/400], Loss: 0.2781\n",
      "Epoch [289/400], Loss: 0.2424\n",
      "Epoch [290/400], Loss: 0.2201\n",
      "Epoch [291/400], Loss: 0.2573\n",
      "Epoch [292/400], Loss: 0.2068\n",
      "Epoch [293/400], Loss: 0.2412\n",
      "Epoch [294/400], Loss: 0.2446\n",
      "Epoch [295/400], Loss: 0.2160\n",
      "Epoch [296/400], Loss: 0.2089\n",
      "Epoch [297/400], Loss: 0.2054\n",
      "Epoch [298/400], Loss: 0.2264\n",
      "Epoch [299/400], Loss: 0.2471\n",
      "Epoch [300/400], Loss: 0.2289\n",
      "Epoch [301/400], Loss: 0.2214\n",
      "Epoch [302/400], Loss: 0.2126\n",
      "Epoch [303/400], Loss: 0.2372\n",
      "Epoch [304/400], Loss: 0.2308\n",
      "Epoch [305/400], Loss: 0.2467\n",
      "Epoch [306/400], Loss: 0.2105\n",
      "Epoch [307/400], Loss: 0.2146\n",
      "Epoch [308/400], Loss: 0.2068\n",
      "Epoch [309/400], Loss: 0.2117\n",
      "Epoch [310/400], Loss: 0.2040\n",
      "Epoch [311/400], Loss: 0.2262\n",
      "Epoch [312/400], Loss: 0.1963\n",
      "Epoch [313/400], Loss: 0.2186\n",
      "Epoch [314/400], Loss: 0.2325\n",
      "Epoch [315/400], Loss: 0.2280\n",
      "Epoch [316/400], Loss: 0.2050\n",
      "Epoch [317/400], Loss: 0.2319\n",
      "Epoch [318/400], Loss: 0.1953\n",
      "Epoch [319/400], Loss: 0.2361\n",
      "Epoch [320/400], Loss: 0.2245\n",
      "Epoch [321/400], Loss: 0.2208\n",
      "Epoch [322/400], Loss: 0.1994\n",
      "Epoch [323/400], Loss: 0.2184\n",
      "Epoch [324/400], Loss: 0.2023\n",
      "Epoch [325/400], Loss: 0.2153\n",
      "Epoch [326/400], Loss: 0.2212\n",
      "Epoch [327/400], Loss: 0.2483\n",
      "Epoch [328/400], Loss: 0.2209\n",
      "Epoch [329/400], Loss: 0.2061\n",
      "Epoch [330/400], Loss: 0.2101\n",
      "Epoch [331/400], Loss: 0.1918\n",
      "Epoch [332/400], Loss: 0.2251\n",
      "Epoch [333/400], Loss: 0.1935\n",
      "Epoch [334/400], Loss: 0.2651\n",
      "Epoch [335/400], Loss: 0.2233\n",
      "Epoch [336/400], Loss: 0.2204\n",
      "Epoch [337/400], Loss: 0.1840\n",
      "Epoch [338/400], Loss: 0.2263\n",
      "Epoch [339/400], Loss: 0.2324\n",
      "Epoch [340/400], Loss: 0.2889\n",
      "Epoch [341/400], Loss: 0.2153\n",
      "Epoch [342/400], Loss: 0.2619\n",
      "Epoch [343/400], Loss: 0.1996\n",
      "Epoch [344/400], Loss: 0.1932\n",
      "Epoch [345/400], Loss: 0.1918\n",
      "Epoch [346/400], Loss: 0.1893\n",
      "Epoch [347/400], Loss: 0.2999\n",
      "Epoch [348/400], Loss: 0.2067\n",
      "Epoch [349/400], Loss: 0.1811\n",
      "Epoch [350/400], Loss: 0.2189\n",
      "Epoch [351/400], Loss: 0.2198\n",
      "Epoch [352/400], Loss: 0.1817\n",
      "Epoch [353/400], Loss: 0.1960\n",
      "Epoch [354/400], Loss: 0.2126\n",
      "Epoch [355/400], Loss: 0.2124\n",
      "Epoch [356/400], Loss: 0.1783\n",
      "Epoch [357/400], Loss: 0.2742\n",
      "Epoch [358/400], Loss: 0.2366\n",
      "Epoch [359/400], Loss: 0.1926\n",
      "Epoch [360/400], Loss: 0.2149\n",
      "Epoch [361/400], Loss: 0.2390\n",
      "Epoch [362/400], Loss: 0.1892\n",
      "Epoch [363/400], Loss: 0.1833\n",
      "Epoch [364/400], Loss: 0.3119\n",
      "Epoch [365/400], Loss: 0.3661\n",
      "Epoch [366/400], Loss: 0.2389\n",
      "Epoch [367/400], Loss: 0.2068\n",
      "Epoch [368/400], Loss: 0.2005\n",
      "Epoch [369/400], Loss: 0.2040\n",
      "Epoch [370/400], Loss: 0.2578\n",
      "Epoch [371/400], Loss: 0.1948\n",
      "Epoch [372/400], Loss: 0.1896\n",
      "Epoch [373/400], Loss: 0.1984\n",
      "Epoch [374/400], Loss: 0.2366\n",
      "Epoch [375/400], Loss: 0.2575\n",
      "Epoch [376/400], Loss: 0.2084\n",
      "Epoch [377/400], Loss: 0.1953\n",
      "Epoch [378/400], Loss: 0.2269\n",
      "Epoch [379/400], Loss: 0.1910\n",
      "Epoch [380/400], Loss: 0.1810\n",
      "Epoch [381/400], Loss: 0.2081\n",
      "Epoch [382/400], Loss: 0.1987\n",
      "Epoch [383/400], Loss: 0.2008\n",
      "Epoch [384/400], Loss: 0.2545\n",
      "Epoch [385/400], Loss: 0.1825\n",
      "Epoch [386/400], Loss: 0.1976\n",
      "Epoch [387/400], Loss: 0.1889\n",
      "Epoch [388/400], Loss: 0.1940\n",
      "Epoch [389/400], Loss: 0.1727\n",
      "Epoch [390/400], Loss: 0.2082\n",
      "Epoch [391/400], Loss: 0.2004\n",
      "Epoch [392/400], Loss: 0.1839\n",
      "Epoch [393/400], Loss: 0.2291\n",
      "Epoch [394/400], Loss: 0.2169\n",
      "Epoch [395/400], Loss: 0.2167\n",
      "Epoch [396/400], Loss: 0.1883\n",
      "Epoch [397/400], Loss: 0.1906\n",
      "Epoch [398/400], Loss: 0.1905\n",
      "Epoch [399/400], Loss: 0.2128\n",
      "Epoch [400/400], Loss: 0.1761\n"
     ]
    }
   ],
   "source": [
    "beng_loss = train_loop(model, \"bengali_model\", train_loader_bengali)\n",
    "#arab_loss = train_loop(model, \"arab_model\")\n",
    "#indo_loss = train_loop(model, \"indo_model\", train_loader_indonesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69cbf6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def evaluate_precision_with_pretrained(model, pretrained_weights_path, val_loader, device):\n",
    "    # Load the pre-trained weights\n",
    "    model.load_state_dict(torch.load(pretrained_weights_path))\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device, dtype=torch.float32)\n",
    "            outputs = model(inputs).squeeze()  # Squeeze the output to remove singleton dimensions\n",
    "\n",
    "            preds = (outputs > 0.5).float()  # Convert probabilities to binary predictions\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1872bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500afd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6d05dcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bengali_precision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#engali_precision = evaluate_precision_with_pretrained(model,'bengali_model',val_loader_bengali, device)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision on validation of Bengali: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbengali_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#ndonesian_precision = evaluate_precision_with_pretrained(model,'indo_model',val_loader_indonesian, device)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision on validation of Indonesian: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindonesian_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bengali_precision' is not defined"
     ]
    }
   ],
   "source": [
    "#engali_precision = evaluate_precision_with_pretrained(model,'bengali_model',val_loader_bengali, device)\n",
    "print(f'Precision on validation of Bengali: {bengali_precision:.4f}')\n",
    "\n",
    "#ndonesian_precision = evaluate_precision_with_pretrained(model,'indo_model',val_loader_indonesian, device)\n",
    "print(f'Precision on validation of Indonesian: {indonesian_precision:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(xs, ys, lang):\n",
    "    flattened_data = np.array([sub_array.flatten() for sub_array in xs])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(flattened_data, ys, test_size=0.1, random_state=42)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, \n",
    "                               scoring='accuracy', cv=kf, n_jobs=-1)\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print('Best Accuracy: ', lang, ' ', grid_search.best_score_ * 100)\n",
    "    #print(f\"Best Accuracy {lang}: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d95b8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#train_rf(bengali_array, benagali_y, 'bengali')\n",
    "#train_rf(indonesian_array, indonesian_y, 'indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51085331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcb97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8044f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5c46b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_xgb_model(xs, ys):\n",
    "    flattened_data = np.array([sub_array.flatten() for sub_array in xs])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(flattened_data, ys, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Define hyperparameters for the XGBoost classifier\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',  # Multiclass classification problem\n",
    "        'num_class': len(set(ys)),    # Number of classes\n",
    "        'n_estimators': 100,           # Number of boosting rounds\n",
    "        'learning_rate': 0.1,         # Learning rate\n",
    "        'max_depth': 6                 # Maximum depth of each tree\n",
    "    }\n",
    "\n",
    "    # Create and fit the XGBoost classifier\n",
    "    classifier = XGBClassifier(**params)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "# Example usage:\n",
    "# trained_classifier = train_xgb_model(xs, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9ea54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_beng = train_xgb_model(bengali_array, benagali_y)#\n",
    "model_indo = train_xgb_model(indonesian_array, indonesian_y)\n",
    "model_arabic = train_xgb_model(arabic_array, arabic_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "718befb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = np.array([sub_array.flatten() for sub_array in bengali_array]) \n",
    "X_train, X_test, y_train, y_test = train_test_split(flattened_data, benagali_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd2b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a75ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def evaluate_xgb_model(xs, ys, lang, model):\n",
    "    flattened_data = np.array([sub_array.flatten() for sub_array in xs])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(flattened_data, ys, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Assuming you have trained and made predictions using your XGBoost model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')  # Precision score\n",
    "    recall = recall_score(y_test, predictions, average='weighted')  # Recall score\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')  # F1 score\n",
    "\n",
    "    print(f\"Test Accuracy ({lang}): {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision ({lang}): {precision:.2f}\")\n",
    "    print(f\"Recall ({lang}): {recall:.2f}\")\n",
    "    print(f\"F1 Score ({lang}): {f1:.2f}\")\n",
    "\n",
    "    return accuracy, precision, f1, lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72e3e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (bengali): 83.33%\n",
      "Precision (bengali): 0.84\n",
      "Recall (bengali): 0.83\n",
      "F1 Score (bengali): 0.83\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_xgb_model(X_test, y_test, 'bengali', model_beng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89aa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc8b1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = np.array([sub_array.flatten() for sub_array in indonesian_array]) \n",
    "X_train, X_test, y_train, y_test = train_test_split(flattened_data, indonesian_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7e91b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (indonesian): 69.30%\n",
      "Precision (indonesian): 0.69\n",
      "Recall (indonesian): 0.69\n",
      "F1 Score (indonesian): 0.69\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_xgb_model(X_test, y_test, 'indonesian', model_indo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18243280",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = np.array([sub_array.flatten() for sub_array in arabic_array]) \n",
    "X_train, X_test, y_train, y_test = train_test_split(flattened_data, arabic_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a756bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (arabic): 80.41%\n",
      "Precision (arabic): 0.81\n",
      "Recall (arabic): 0.80\n",
      "F1 Score (arabic): 0.80\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_xgb_model(X_test, y_test, 'arabic', model_arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b716d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
