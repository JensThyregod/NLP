{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cfd522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import Preprocessor as p\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch.nn.functional as  F\n",
    "import csv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from next_word_prediction import GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dbf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ef5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the copenlu/answerable_tydiqa dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "train_df = train_df[train_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "train_df = p.DataFramePreprocessor(train_df).df\n",
    "\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "val_df = val_df[val_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "val_df = p.DataFramePreprocessor(val_df).df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a04dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df\n",
    "data['document_length'] = data['document_plaintext'].str.len()\n",
    "data['is_answerable'] = data['annotations'].apply(lambda x: x.get('answer_start', [-1])[0] != -1)\n",
    "\n",
    "val_df['is_answerable'] = data['annotations'].apply(lambda x: x.get('answer_start', [-1])[0] != -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d54b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275424f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset = ['question_text'])\n",
    "data = data.drop_duplicates(subset = ['document_plaintext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e491b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d18f999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali = data[data['language'] == 'bengali']\n",
    "arabic = data[data['language'] == 'arabic']\n",
    "indonesian = data[data['language'] == 'indonesian']\n",
    "\n",
    "\n",
    "bengali_val = val_df[val_df['language'] == 'bengali']\n",
    "arabic_val = val_df[val_df['language'] == 'arabic']\n",
    "indonesian_val = val_df[val_df['language'] == 'indonesian']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64532a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1191"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indonesian_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d341ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\\nfrom datasets import Dataset\\n\\n# Load the GPT-2 tokenizer and model\\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\\n\\n# Sample pandas DataFrame with a column named \"text\"\\n# Replace this with your actual pandas DataFrame\\ndata = arabic\\n\\n# Create a custom dataset using the datasets library\\ncustom_dataset = Dataset.from_pandas(data)\\n\\n# Tokenize the custom dataset\\ndef tokenize_function(examples):\\n    return tokenizer(examples[\"question_text\"])\"\\ntokenized_datasets = custom_dataset.map(tokenize_function, batched=True)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Sample pandas DataFrame with a column named \"text\"\n",
    "# Replace this with your actual pandas DataFrame\n",
    "data = arabic\n",
    "\n",
    "# Create a custom dataset using the datasets library\n",
    "custom_dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Tokenize the custom dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"question_text\"])\"\n",
    "tokenized_datasets = custom_dataset.map(tokenize_function, batched=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8800706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5974b46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from transformers import Trainer, TrainingArguments\\n\\n# Define training arguments\\ntraining_args = TrainingArguments(\\n    output_dir=\"./results\",  # Output directory\\n    num_train_epochs=3,      # Number of training epochs\\n    per_device_train_batch_size=4,  # Batch size per device during training\\n    save_steps=500,          # Save checkpoint every X steps\\n    logging_steps=100,       # Log metrics every X steps\\n)\\n\\n# Instantiate Trainer\\ntrainer = Trainer(\\n    model=model,                     # The model to be trained\\n    args=training_args,              # Training arguments\\n    train_dataset=tokenized_datasets, # Training dataset\\n)\\n\\n# Fine-tune the model\\ntrainer.train()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    num_train_epochs=3,      # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size per device during training\n",
    "    save_steps=500,          # Save checkpoint every X steps\n",
    "    logging_steps=100,       # Log metrics every X steps\n",
    ")\n",
    "\n",
    "# Instantiate Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                     # The model to be trained\n",
    "    args=training_args,              # Training arguments\n",
    "    train_dataset=tokenized_datasets, # Training dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2849a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 words: [('second', 0.07366503775119781), ('word:', 0.060055650770664215), ('word.', 0.060055650770664215)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a function to generate top N words from a sequence\n",
    "def generate_top_words(sequence, n=3):\n",
    "    input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text from the model\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract top N words with highest probabilities\n",
    "    words = generated_text.split()\n",
    "    word_probabilities = []\n",
    "    for word in words:\n",
    "        word_id = tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "        word_probability = torch.softmax(model(torch.tensor([[word_id]]))[0][:, -1], dim=1).max().item()\n",
    "        word_probabilities.append((word, word_probability))\n",
    "    top_words = sorted(word_probabilities, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    return top_words\n",
    "\n",
    "# Define a prompt for generating a random word\n",
    "prompt = \"Generate a random word: \"\n",
    "\n",
    "# Generate and print top 3 words\n",
    "top_words = generate_top_words(prompt)\n",
    "print(f\"Top 3 words: {top_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "175303fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[317], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m translated\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Apply translation function to the 'arabic_text' column and create a new 'english_translation' column\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m arabic_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish_translation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43marabic_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslate_to_english\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Print the DataFrame with English translations\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[317], line 7\u001b[0m, in \u001b[0;36mtranslate_to_english\u001b[0;34m(arabic_text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_to_english\u001b[39m(arabic_text):\n\u001b[1;32m      6\u001b[0m     translator \u001b[38;5;241m=\u001b[39m Translator()\n\u001b[0;32m----> 7\u001b[0m     translated \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marabic_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Translate from Arabic to English\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m translated\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/googletrans/client.py:210\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    209\u001b[0m origin \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m--> 210\u001b[0m data, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# this code will be updated when the format is changed.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/googletrans/client.py:108\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    104\u001b[0m params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type, query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[1;32m    105\u001b[0m                             token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[1;32m    107\u001b[0m url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n\u001b[0;32m--> 108\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    111\u001b[0m     data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mformat_json(r\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_client.py:755\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, url, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    746\u001b[0m     url: URLTypes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    753\u001b[0m     timeout: typing\u001b[38;5;241m.\u001b[39mUnion[TimeoutTypes, UnsetType] \u001b[38;5;241m=\u001b[39m UNSET,\n\u001b[1;32m    754\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_client.py:600\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, data, files, json, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    577\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: typing\u001b[38;5;241m.\u001b[39mUnion[TimeoutTypes, UnsetType] \u001b[38;5;241m=\u001b[39m UNSET,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    590\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    591\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    592\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m         cookies\u001b[38;5;241m=\u001b[39mcookies,\n\u001b[1;32m    599\u001b[0m     )\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_client.py:620\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    616\u001b[0m timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, UnsetType) \u001b[38;5;28;01melse\u001b[39;00m Timeout(timeout)\n\u001b[1;32m    618\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_auth(request, auth)\n\u001b[0;32m--> 620\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_client.py:647\u001b[0m, in \u001b[0;36mClient.send_handling_redirects\u001b[0;34m(self, request, auth, timeout, allow_redirects, history)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_redirects:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRedirects()\n\u001b[0;32m--> 647\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m response\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(history)\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mis_redirect:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_client.py:684\u001b[0m, in \u001b[0;36mClient.send_handling_auth\u001b[0;34m(self, request, history, auth, timeout)\u001b[0m\n\u001b[1;32m    682\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 684\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m auth\u001b[38;5;241m.\u001b[39mrequires_response_body:\n\u001b[1;32m    686\u001b[0m         response\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_client.py:714\u001b[0m, in \u001b[0;36mClient.send_single_request\u001b[0;34m(self, request, timeout)\u001b[0m\n\u001b[1;32m    705\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_for_url(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    708\u001b[0m     (\n\u001b[1;32m    709\u001b[0m         http_version,\n\u001b[1;32m    710\u001b[0m         status_code,\n\u001b[1;32m    711\u001b[0m         reason_phrase,\n\u001b[1;32m    712\u001b[0m         headers,\n\u001b[1;32m    713\u001b[0m         stream,\n\u001b[0;32m--> 714\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Add the original request to any HTTPError unless\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# there'a already a request attached in the case of\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m# a ProxyError.\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m_request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py:152\u001b[0m, in \u001b[0;36mSyncConnectionPool.request\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    149\u001b[0m         logger\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreuse connection=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NewConnectionRequired:\n\u001b[1;32m    156\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mSyncHTTPConnection.request\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     75\u001b[0m logger\u001b[38;5;241m.\u001b[39mtrace(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection.request method=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m url=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m headers=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, method, url, headers\n\u001b[1;32m     77\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http2.py:118\u001b[0m, in \u001b[0;36mSyncHTTP2Connection.request\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreams[stream_id] \u001b[38;5;241m=\u001b[39m h2_stream\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents[stream_id] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh2_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_streams_semaphore\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http2.py:292\u001b[0m, in \u001b[0;36mSyncHTTP2Stream.request\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_body(stream, timeout)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Receive the response.\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m status_code, headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m reason_phrase \u001b[38;5;241m=\u001b[39m get_reason_phrase(status_code)\n\u001b[1;32m    294\u001b[0m stream \u001b[38;5;241m=\u001b[39m SyncByteStream(\n\u001b[1;32m    295\u001b[0m     iterator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody_iter(timeout), close_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed\n\u001b[1;32m    296\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http2.py:344\u001b[0m, in \u001b[0;36mSyncHTTP2Stream.receive_response\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03mRead the response status and headers from the network.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h2\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mResponseReceived):\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http2.py:197\u001b[0m, in \u001b[0;36mSyncHTTP2Connection.wait_for_event\u001b[0;34m(self, stream_id, timeout)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_lock:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents[stream_id]:\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents[stream_id]\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http2.py:204\u001b[0m, in \u001b[0;36mSyncHTTP2Connection.receive_events\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreceive_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: TimeoutDict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Read some data from the network, and update the H2 state.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh2_state\u001b[38;5;241m.\u001b[39mreceive_data(data)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_backends/sync.py:62\u001b[0m, in \u001b[0;36mSyncSocketStream.read\u001b[0;34m(self, n, timeout)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39msettimeout(read_timeout)\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1226\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1225\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1101\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "# Function to translate Arabic text to English using Google Translate API\n",
    "def translate_to_english(arabic_text):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(arabic_text, src='ar', dest='en')  # Translate from Arabic to English\n",
    "    return translated.text\n",
    "\n",
    "# Apply translation function to the 'arabic_text' column and create a new 'english_translation' column\n",
    "arabic_val['english_translation'] = arabic_val['question_text'].apply(translate_to_english)\n",
    "\n",
    "# Print the DataFrame with English translations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "9e042e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate Arabic text to English using Google Translate API\n",
    "def translate_to_english(arabic_text):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(arabic_text, src='ar', dest='en')  # Translate from Arabic to English\n",
    "    return translated.text\n",
    "\n",
    "# Apply translation function to the 'arabic_text' column and create a new 'english_translation' column\n",
    "arabic_val['english_translation_doc'] = arabic_val['document_plaintext'].apply(translate_to_english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718b56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "da72bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_english(arabic_text):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(arabic_text, src='bn', dest='en')  # Translate from Arabic to English\n",
    "    return translated.text\n",
    "\n",
    "#bengali_val['english_translation'] = bengali_val['question_text'].apply(translate_to_english)\n",
    "bengali_val['english_translation_doc'] = bengali_val['document_plaintext'].apply(translate_to_english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62707695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "e03ee464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_english(arabic_text):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(arabic_text, src='id', dest='en')  # Translate from Arabic to English\n",
    "    return translated.text\n",
    "\n",
    "\n",
    "#indonesian_val['english_translation'] = indonesian_val['question_text'].apply(translate_to_english)\n",
    "indonesian_val['english_translation_doc'] = indonesian_val['document_plaintext'].apply(translate_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8e7c40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indonesian_val.to_csv('indonesian_val.csv', index=True)\n",
    "bengali_val.to_csv('bengali_val.csv', index=True)\n",
    "arabic_val.to_csv('arabic_val.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ca2465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2()\n",
    "def predict_next_word(sentence, word, top_n=3):\n",
    "    # Your prediction logic here, return top_n predictions for the word\n",
    "    # Example: Replace this line with your prediction logic\n",
    "    predictions = gpt2.predict_next(word, top_n)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef8572e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Import CSV files into pandas dataframes\n",
    "indonesian_val = pd.read_csv('indonesian_val.csv', index_col=0)\n",
    "bengali_val = pd.read_csv('bengali_val.csv', index_col=0)\n",
    "arabic_val = pd.read_csv('arabic_val.csv', index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the 'english_translation' column into normal tokens\n",
    "bengali_val['tokenized_english_translation'] = bengali_val['english_translation_doc'].apply(lambda x: nltk.word_tokenize(x))\n",
    "indonesian_val['tokenized_english_translation'] = indonesian_val['english_translation_doc'].apply(lambda x: nltk.word_tokenize(x))\n",
    "arabic_val['tokenized_english_translation'] = arabic_val['english_translation_doc'].apply(lambda x: nltk.word_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b95a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tokenized_sentences = arabic_val['tokenized_english_translation']\\npredictions_data = generate_predictions_dataframe(tokenized_sentences)\\npredictions_dataframe_arabic = pd.DataFrame(predictions_data, columns=['Word', 'Predictions'])\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_predictions_dataframe(tokenized_sentences):\n",
    "    predictions_list = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        context = []  # Keep track of previous words as context\n",
    "        sentence_predictions = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            predictions = predict_next_word(context, word, top_n=3)\n",
    "            sentence_predictions.append((word, predictions))\n",
    "            context.append(word)  # Update context with the current word\n",
    "        \n",
    "        predictions_list.extend(sentence_predictions)\n",
    "    \n",
    "    return predictions_list\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\"\"\"tokenized_sentences = arabic_val['tokenized_english_translation']\n",
    "predictions_data = generate_predictions_dataframe(tokenized_sentences)\n",
    "predictions_dataframe_arabic = pd.DataFrame(predictions_data, columns=['Word', 'Predictions'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e9f452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = indonesian_val['tokenized_english_translation']\n",
    "predictions_data = generate_predictions_dataframe(tokenized_sentences)\n",
    "predictions_dataframe_indonesian = pd.DataFrame(predictions_data, columns=['Word', 'Predictions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f838fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = bengali_val['tokenized_english_translation']\n",
    "predictions_data = generate_predictions_dataframe(tokenized_sentences)\n",
    "predictions_dataframe_bengali = pd.DataFrame(predictions_data, columns=['Word', 'Predictions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676185d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ab8d6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.5, 0.5, 0.5, 0.5]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bengali_avg = get_acc(predictions_dataframe_bengali)\n",
    "bengali_avg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Indo docs: \n",
    "\n",
    "[0.0073,\n",
    " 0.9984,\n",
    " 0.4897,\n",
    " 0.0912,\n",
    " 0.0167,\n",
    " 0.0293,\n",
    " 0.0466,\n",
    " 0.0262,\n",
    " 0.0204,\n",
    " 0.01153]\n",
    "\n",
    "arabisk doc \n",
    "\n",
    "[0.3166,\n",
    " 0.6421,\n",
    " 0.6142,\n",
    " 0.4166,\n",
    " 0.5613,\n",
    " 0.6123,\n",
    " 0.5321,\n",
    " 0.5234,\n",
    " 0.3543,\n",
    " 0.2324]\n",
    "\n",
    "bengali doc\n",
    "\n",
    "[0.2234, 0.5882, 0.4166, 0.3231, 0.6123, 0.3315, 0.3413, 0.5164, 0.5435, 0.5319]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a43abbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9166666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.4166666666666667,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 0.3333333333333333,\n",
       " 0.75]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indonesian_avg = get_acc(predictions_dataframe_indonesian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9405eaae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_dataframe_arabic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m arabic_avg \u001b[38;5;241m=\u001b[39m get_acc(\u001b[43mpredictions_dataframe_arabic\u001b[49m)\n\u001b[1;32m      2\u001b[0m arabic_avg[:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_dataframe_arabic' is not defined"
     ]
    }
   ],
   "source": [
    "arabic_avg = get_acc(predictions_dataframe_arabic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996651a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5685b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tokenized_sentences = arabic_val['tokenized_english']\\npredictions_data = generate_predictions_dataframe(tokenized_sentences)\\n\\n# Convert to DataFrame\\npredictions_dataframe_arabic = pd.DataFrame(predictions_data, columns=['Word', 'Predictions'])\\n\\n# Print or return the predictions DataFrame\\nprint(predictions_dataframe)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_predictions_dataframe(tokenized_sentences):\n",
    "    predictions_list = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        context = []  # Keep track of previous words as context\n",
    "        sentence_predictions = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            predictions = predict_next_word(context, word, top_n=3)\n",
    "            sentence_predictions.append((word, predictions))\n",
    "            context.append(word)  # Update context with the current word\n",
    "        \n",
    "        predictions_list.extend(sentence_predictions)\n",
    "    \n",
    "    return predictions_list\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\"\"\"tokenized_sentences = arabic_val['tokenized_english']\n",
    "predictions_data = generate_predictions_dataframe(tokenized_sentences)\n",
    "\n",
    "# Convert to DataFrame\n",
    "predictions_dataframe_arabic = pd.DataFrame(predictions_data, columns=['Word', 'Predictions'])\n",
    "\n",
    "# Print or return the predictions DataFrame\n",
    "print(predictions_dataframe)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ea29d1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        [How, many, times, has, Uruguay, won, the, FIF...\n",
       "6        [Who, is, the, discoverer, of, quartz, or, qua...\n",
       "18       [How, does, the, fetus, connect, to, the, uter...\n",
       "21       [Where, is, Sultan, Abdul, Majeed, Mosque, loc...\n",
       "24                [What, is, the, capital, of, Georgia, ?]\n",
       "                               ...                        \n",
       "13300    [How, many, counties, is, Liberia, divided, in...\n",
       "13304                 [When, was, toothpaste, invented, ?]\n",
       "13308               [What, is, existential, philosophy, ?]\n",
       "13312        [How, high, is, the, High, Dam, in, Egypt, ?]\n",
       "13324     [What, is, my, first, author, ?, K., Rowling, ?]\n",
       "Name: tokenized_english, Length: 1902, dtype: object"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dataframe_arabic\n",
    "predictions_dataframe_bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "03ca2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = bengali_val['tokenized_english']\n",
    "predictions_data = generate_predictions_dataframe(tokenized_sentences)\n",
    "\n",
    "# Convert to DataFrame\n",
    "predictions_dataframe_bengali = pd.DataFrame(predictions_data, columns=['Word', 'Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c5eac8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = indonesian_val['tokenized_english']\n",
    "predictions_data = generate_predictions_dataframe(tokenized_sentences)\n",
    "\n",
    "# Convert to DataFrame\n",
    "predictions_dataframe_indonesian = pd.DataFrame(predictions_data, columns=['Word', 'Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "9aa0ab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>result</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who</td>\n",
       "      <td>[is, 's, are]</td>\n",
       "      <td>0</td>\n",
       "      <td>discovered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>discovered</td>\n",
       "      <td>[the, this, that]</td>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>[secret, truth, new]</td>\n",
       "      <td>0</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American</td>\n",
       "      <td>[Revolution, dream, Dream]</td>\n",
       "      <td>0</td>\n",
       "      <td>continent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>continent</td>\n",
       "      <td>[was, in, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9747</th>\n",
       "      <td>largest</td>\n",
       "      <td>[city, company, of]</td>\n",
       "      <td>0</td>\n",
       "      <td>manta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9748</th>\n",
       "      <td>manta</td>\n",
       "      <td>[ray, rays, -]</td>\n",
       "      <td>0</td>\n",
       "      <td>ray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>ray</td>\n",
       "      <td>[cost, have, make]</td>\n",
       "      <td>0</td>\n",
       "      <td>weigh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>weigh</td>\n",
       "      <td>[?, in, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;\\EOF&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751</th>\n",
       "      <td>&lt;\\EOF&gt;</td>\n",
       "      <td>[, The, How]</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9752 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word                 Predictions  result        word\n",
       "0            Who               [is, 's, are]       0  discovered\n",
       "1     discovered           [the, this, that]       0         the\n",
       "2            the        [secret, truth, new]       0    American\n",
       "3       American  [Revolution, dream, Dream]       0   continent\n",
       "4      continent                [was, in, ,]       1           ?\n",
       "...          ...                         ...     ...         ...\n",
       "9747     largest         [city, company, of]       0       manta\n",
       "9748       manta              [ray, rays, -]       0         ray\n",
       "9749         ray          [cost, have, make]       0       weigh\n",
       "9750       weigh                  [?, in, ,]       0      <\\EOF>\n",
       "9751      <\\EOF>                [, The, How]       1        None\n",
       "\n",
       "[9752 rows x 4 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221d182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a56c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eaea4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_list(row):\n",
    "    return int(any(word in row['Word'] for word in row['Predictions']))\n",
    "\n",
    "def check_word_list_nan(row):\n",
    "    if '?' in row['Word']:\n",
    "        return np.nan\n",
    "    return int(any(word in row['Word'] for word in row['Predictions']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a4a1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(df):\n",
    "    df = df\n",
    "    df['Word'] = df['Word'].shift(-1)\n",
    "    df.at[df.index[-1], 'Word'] = '<\\\\EOF>'\n",
    "    # Apply the function to each row and store the result in a new column 'result'\n",
    "    \n",
    "    df['result'] = df.apply(lambda row: check_word_list(row), axis=1)\n",
    "    \n",
    "    # Apply the function to each row and store the result in a new column 'result'\n",
    "    df['result'] = df.apply(lambda row: check_word_list_nan(row), axis=1)\n",
    "    # Split the 'result' column for every NaN value and append the sequence of 0s and 1s to a list\n",
    "    split_results = []\n",
    "    current_sequence = []\n",
    "    for value in df['result']:\n",
    "        if pd.isna(value):\n",
    "            if current_sequence:\n",
    "                split_results.append(current_sequence)\n",
    "                current_sequence = []\n",
    "        else:\n",
    "            current_sequence.append(value)\n",
    "\n",
    "    # Append the last sequence if it exists\n",
    "    if current_sequence:\n",
    "        split_results.append(current_sequence)\n",
    "\n",
    "     # Your list of lists\n",
    "    lists = split_results.copy()\n",
    "\n",
    "    # Get the length of the longest sublist\n",
    "    max_length = max(len(sublist) for sublist in lists)\n",
    "\n",
    "    # Pad the shorter sublists with 0.0 values to make them equal in length\n",
    "    padded_lists = [sublist + [0.0] * (max_length - len(sublist)) for sublist in lists]\n",
    "\n",
    "    # Calculate the average of every position in the lists\n",
    "    averages = [sum(col) / len(col) for col in zip(*padded_lists)]\n",
    "\n",
    "    return averages\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "5ccdea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_avg = get_acc(predictions_dataframe_bengali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "e599edb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9777777777777777,\n",
       " 0.5466666666666666,\n",
       " 0.6133333333333333,\n",
       " 0.2311111111111111,\n",
       " 0.4888888888888889,\n",
       " 0.4533333333333333,\n",
       " 0.29777777777777775,\n",
       " 0.26222222222222225,\n",
       " 0.17333333333333334,\n",
       " 0.13333333333333333]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bengali_avg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4ede5670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010067114093959731,\n",
       " 0.9991610738255033,\n",
       " 0.4354026845637584,\n",
       " 0.07718120805369127,\n",
       " 0.0436241610738255,\n",
       " 0.05201342281879195,\n",
       " 0.053691275167785234,\n",
       " 0.04697986577181208,\n",
       " 0.02181208053691275,\n",
       " 0.006711409395973154]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indonesian_avg = get_acc(predictions_dataframe_indonesian)\n",
    "indonesian_avg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "140cc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_avg = get_acc(predictions_dataframe_arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "10233d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9994756161510225,\n",
       " 0.7126376507603566,\n",
       " 0.47614053487152597,\n",
       " 0.1740954378605139,\n",
       " 0.33560566334556896,\n",
       " 0.281069743051914,\n",
       " 0.1976927110644992,\n",
       " 0.14735186156266386,\n",
       " 0.11955951756685894,\n",
       " 0.07813319349764027]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_avg = get_acc(predictions_dataframe_arabic)\n",
    "arabic_avg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c69b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28da60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e866c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0f27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f63614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0b495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd6a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09194eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584ff11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc5b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22212f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02dc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7f663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271f332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
