{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "libraries and dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "dir_path = os.getcwd()[:-1]+'6'\n",
    "\n",
    "train_df = pd.read_csv(dir_path+'/train_data.csv')\n",
    "val_df = pd.read_csv(dir_path+'/train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module): # inherit nn.Module class\n",
    "    def __init__(self, input_d, hidden_d, layer_d, output_d):\n",
    "        \"\"\"\n",
    "        input_d : the number of expected features in the input\n",
    "        hidden_d: the number of features in the hidden state.\n",
    "        layer_d : the number of layers \n",
    "        output_d: the number of output nodes\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_d # the hidden d is the deadliest\n",
    "        self.layer_dim = layer_d\n",
    "\n",
    "        # LSTM model \n",
    "        self.lstm = nn.LSTM(input_d, hidden_d, layer_d, batch_first=True) \n",
    "\n",
    "        # fully connected\n",
    "        self.fc = nn.Linear(hidden_d, output_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # This step takes place input_d times. We detach as we truncate Backpropagation\n",
    "        # through time (BPTT). If we don't detach, we'll backprop to the start.\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 30\n",
    "hidden_dim = 120\n",
    "output_dim = 15\n",
    "layer_dim = 1\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4: calculating cross entropy loss\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "#step 5: optimizer \n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fb1f590>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import Preprocessor as p\n",
    "import math\n",
    "\n",
    "#import torchtext\n",
    "#import datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing punctuation with tokens...\n",
      "Replacing punctuation with tokens...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dir_path = os.getcwd()[:-1]+'6'\n",
    "train_df = pd.read_csv(dir_path+'/train_data.csv')\n",
    "val_df = pd.read_csv(dir_path+'/train_data.csv')\n",
    "\n",
    "train_df = p.DataFramePreprocessor(train_df, \n",
    "                                   ['question_text','document_plaintext'], \n",
    "                                   remove_stopwords=False, \n",
    "                                   remove_punctuation=False, \n",
    "                                   add_special_tokens=False,\n",
    "                                   count=False)\n",
    "val_df = p.DataFramePreprocessor(val_df, \n",
    "                                   ['question_text','document_plaintext'], \n",
    "                                   remove_stopwords=False, \n",
    "                                   remove_punctuation=False, # tokenize punctuation\n",
    "                                   add_special_tokens=False,\n",
    "                                   count=False)\n",
    "\n",
    "# divide data set\n",
    "train_arab = train_df.df[train_df.df['language'] == 'arabic']\n",
    "train_indo = train_df.df[train_df.df['language'] == 'indonesian']\n",
    "train_beng = train_df.df[train_df.df['language'] == 'bengali']\n",
    "\n",
    "val_arab = val_df.df[val_df.df['language'] == 'arabic']\n",
    "val_indo = val_df.df[val_df.df['language'] == 'indonesian']\n",
    "val_beng = val_df.df[val_df.df['language'] == 'bengali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>question_text_tokens</th>\n",
       "      <th>document_plaintext_tokens</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dimanakah  Dr. Ernest François Eugène Douwes D...</td>\n",
       "      <td>Ernest Douwes Dekker wafat dini hari tanggal 2...</td>\n",
       "      <td>[dimanakah, dr., ernest, françois, eugène, dou...</td>\n",
       "      <td>[ernest, douwes, dekker, wafat, dini, hari, ta...</td>\n",
       "      <td>['ernest', 'douwes', 'dekker', 'wafat', 'tangg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Siapa arsitek Balai Kota Seoul?</td>\n",
       "      <td>Pada tanggal 18 Februari 2008, desain Yoo Kerl...</td>\n",
       "      <td>[siapa, arsitek, balai, kota, seoul?, &lt;EOS&gt;]</td>\n",
       "      <td>[pada, tanggal, 18, februari, 2008,, desain, y...</td>\n",
       "      <td>['tanggal', '18', 'februari', '2008', 'desain'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kapan PBB mulai terbentuk ?</td>\n",
       "      <td>Sebagai tindak lanjut Atlantic Charter tersebu...</td>\n",
       "      <td>[kapan, pbb, mulai, terbentuk, ?, &lt;EOS&gt;]</td>\n",
       "      <td>[sebagai, tindak, lanjut, atlantic, charter, t...</td>\n",
       "      <td>['tindak', 'atlantic', 'charter', 'tanggal', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dimana James Hepburn meninggal?</td>\n",
       "      <td>Dia dipenjarakan di Puri Dragsholm, 75 kilomet...</td>\n",
       "      <td>[dimana, james, hepburn, meninggal?, &lt;EOS&gt;]</td>\n",
       "      <td>[dia, dipenjarakan, di, puri, dragsholm,, 75, ...</td>\n",
       "      <td>['dipenjarakan', 'puri', 'dragsholm', '75', 'k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dimana Jamie Richard Vardy lahir?</td>\n",
       "      <td>Lahir di Sheffield, South Yorkshire, Vardy mem...</td>\n",
       "      <td>[dimana, jamie, richard, vardy, lahir?, &lt;EOS&gt;]</td>\n",
       "      <td>[lahir, di, sheffield,, south, yorkshire,, var...</td>\n",
       "      <td>['lahir', 'sheffield', 'south', 'yorkshire', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_text  \\\n",
       "2   dimanakah  Dr. Ernest François Eugène Douwes D...   \n",
       "4                     Siapa arsitek Balai Kota Seoul?   \n",
       "9                         Kapan PBB mulai terbentuk ?   \n",
       "14                    Dimana James Hepburn meninggal?   \n",
       "18                  Dimana Jamie Richard Vardy lahir?   \n",
       "\n",
       "                                   document_plaintext  \\\n",
       "2   Ernest Douwes Dekker wafat dini hari tanggal 2...   \n",
       "4   Pada tanggal 18 Februari 2008, desain Yoo Kerl...   \n",
       "9   Sebagai tindak lanjut Atlantic Charter tersebu...   \n",
       "14  Dia dipenjarakan di Puri Dragsholm, 75 kilomet...   \n",
       "18  Lahir di Sheffield, South Yorkshire, Vardy mem...   \n",
       "\n",
       "                                 question_text_tokens  \\\n",
       "2   [dimanakah, dr., ernest, françois, eugène, dou...   \n",
       "4        [siapa, arsitek, balai, kota, seoul?, <EOS>]   \n",
       "9            [kapan, pbb, mulai, terbentuk, ?, <EOS>]   \n",
       "14        [dimana, james, hepburn, meninggal?, <EOS>]   \n",
       "18     [dimana, jamie, richard, vardy, lahir?, <EOS>]   \n",
       "\n",
       "                            document_plaintext_tokens  \\\n",
       "2   [ernest, douwes, dekker, wafat, dini, hari, ta...   \n",
       "4   [pada, tanggal, 18, februari, 2008,, desain, y...   \n",
       "9   [sebagai, tindak, lanjut, atlantic, charter, t...   \n",
       "14  [dia, dipenjarakan, di, puri, dragsholm,, 75, ...   \n",
       "18  [lahir, di, sheffield,, south, yorkshire,, var...   \n",
       "\n",
       "                                               tokens  \n",
       "2   ['ernest', 'douwes', 'dekker', 'wafat', 'tangg...  \n",
       "4   ['tanggal', '18', 'februari', '2008', 'desain'...  \n",
       "9   ['tindak', 'atlantic', 'charter', 'tanggal', '...  \n",
       "14  ['dipenjarakan', 'puri', 'dragsholm', '75', 'k...  \n",
       "18  ['lahir', 'sheffield', 'south', 'yorkshire', '...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indo[['question_text','document_plaintext','question_text_tokens','document_plaintext_tokens','tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/bh2csj4147zbdsr0w_k_qr4m0000gn/T/ipykernel_87376/3097707873.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_indo['question_text_tokens'] =  train_indo['question_text'].apply(tokenize)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"replace punctuation with tokens\"\"\"\n",
    "    text = text.replace(',', ' <COM> ')\n",
    "    text = text.replace('.', ' <PUN> ')\n",
    "    text = text.replace(':', ' <COL> ')\n",
    "    text = text.replace(';', ' <SEM> ')\n",
    "    text = text.replace('(', ' <LPA> ')\n",
    "    text = text.replace(')', ' <RPA> ')\n",
    "    text = text.replace('?', ' <QUE> ')\n",
    "    text = text.replace('!', ' <EXC> ')\n",
    "    text = text.replace('\"',  '<QUO> ')\n",
    "    text = text.replace('\\'', '')\n",
    "    return text\n",
    "\n",
    "train_indo['question_text_tokens'] =  train_indo['question_text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vocabulary (tf-idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader \n",
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:                                      \n",
    "            tokens = example['tokens'].append('<eos>')        # add end-of-string token     \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_layers: int, \n",
    "                 dropout_rate: float, tie_weights: bool):\n",
    "        \"\"\"\n",
    "        vocab_size: size of one-hot vector\n",
    "        embedding_dim: dimension of the word representation.\n",
    "        hidden_dim: network width\n",
    "        num_layers: network depth\n",
    "        dropout_rate: regularization method\n",
    "        tie_weights: Weight tying is a method that dispenses with this redundancy and \n",
    "            simply uses a single set of embeddings at the input and softmax layers. That \n",
    "            is, we dispense with V and use E in both the start and end of the computation.\n",
    "            In addition to providing improved model perplexity, this approach significantly \n",
    "            reduces the number of parameters required for the model.\n",
    "\n",
    "        \"\"\"\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
    "            self.embedding.weight = self.fc.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)          \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_batch(data, seq_len, num_batches, idx):\n",
    "    src = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]             \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)   \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad(): # we no longer need to backprop or keep track of gradients.\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = \n",
    "seq_len = \n",
    "clip = \n",
    "saved = True\n",
    "\n",
    "# reduce learning rate as we go along \n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0) \n",
    "\n",
    "if saved:\n",
    "    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
    "else:\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_data, optimizer, criterion, \n",
    "                    batch_size, seq_len, clip, device)\n",
    "        valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                    seq_len, device)\n",
    "        \n",
    "        lr_scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']:\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(prediction)\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "prompt = 'Think about'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
