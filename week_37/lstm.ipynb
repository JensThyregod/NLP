{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "libraries and dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "dir_path = os.getcwd()[:-1]+'6'\n",
    "\n",
    "train_df = pd.read_csv(dir_path+'/train_data.csv')\n",
    "val_df = pd.read_csv(dir_path+'/train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module): # inherit nn.Module class\n",
    "    def __init__(self, input_d, hidden_d, layer_d, output_d):\n",
    "        \"\"\"\n",
    "        input_d : the number of expected features in the input\n",
    "        hidden_d: the number of features in the hidden state.\n",
    "        layer_d : the number of layers \n",
    "        output_d: the number of output nodes\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_d # the hidden d is the deadliest\n",
    "        self.layer_dim = layer_d\n",
    "\n",
    "        # LSTM model \n",
    "        self.lstm = nn.LSTM(input_d, hidden_d, layer_d, batch_first=True) \n",
    "\n",
    "        # fully connected\n",
    "        self.fc = nn.Linear(hidden_d, output_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # This step takes place input_d times. We detach as we truncate Backpropagation\n",
    "        # through time (BPTT). If we don't detach, we'll backprop to the start.\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 30\n",
    "hidden_dim = 120\n",
    "output_dim = 15\n",
    "layer_dim = 1\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4: calculating cross entropy loss\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "#step 5: optimizer \n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bpemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fe1b5d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import Preprocessor as p\n",
    "import math\n",
    "import functools as ft\n",
    "import operator\n",
    "from bpemb import BPEmb\n",
    "\n",
    "#import torchtext\n",
    "#import datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing punctuation with tokens...\n",
      "Replacing punctuation with tokens...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dir_path = os.getcwd()[:-1]+'6'\n",
    "train_df = pd.read_csv(dir_path+'/train_data.csv')\n",
    "val_df = pd.read_csv(dir_path+'/train_data.csv')\n",
    "\n",
    "# Preprocess\n",
    "train_df = p.DataFramePreprocessor(train_df, \n",
    "                                   ['question_text','document_plaintext'], \n",
    "                                   remove_stopwords=False, \n",
    "                                   remove_punctuation=False, \n",
    "                                   add_special_tokens=False,\n",
    "                                   count=False)\n",
    "val_df = p.DataFramePreprocessor(val_df, \n",
    "                                   ['question_text','document_plaintext'], \n",
    "                                   remove_stopwords=False, \n",
    "                                   remove_punctuation=False, # tokenize punctuation\n",
    "                                   add_special_tokens=False,\n",
    "                                   count=False)\n",
    "\n",
    "# divide data set\n",
    "train_arab = train_df.df[train_df.df['language'] == 'arabic']\n",
    "train_indo = train_df.df[train_df.df['language'] == 'indonesian']\n",
    "train_beng = train_df.df[train_df.df['language'] == 'bengali']\n",
    "\n",
    "val_arab = val_df.df[val_df.df['language'] == 'arabic']\n",
    "val_indo = val_df.df[val_df.df['language'] == 'indonesian']\n",
    "val_beng = val_df.df[val_df.df['language'] == 'bengali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader \n",
    "def get_data(dataset: list, vocab, batch_size):\n",
    "    \"\"\"\n",
    "    dataset: pandas series\n",
    "    \"\"\"\n",
    "    encoded = vocab.encode_ids_with_eos(dataset.values)\n",
    "    flat = ft.reduce(operator.iconcat, encoded, [])\n",
    "    data = torch.tensor(flat)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size] # remove modulus (leftovers)\n",
    "    data = data.view(batch_size, num_batches) \n",
    "    return data\n",
    "\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_layers: int, \n",
    "                 dropout_rate: float, tie_weights: bool, vocab):\n",
    "        \"\"\"\n",
    "        vocab_size: size of one-hot vector\n",
    "        embedding_dim: dimension of the word representation.\n",
    "        hidden_dim: network width\n",
    "        num_layers: network depth\n",
    "        dropout_rate: regularization method\n",
    "        tie_weights: Weight tying is a method that dispenses with this redundancy and \n",
    "            simply uses a single set of embeddings at the input and softmax layers. That \n",
    "            is, we dispense with V and use E in both the start and end of the computation.\n",
    "            In addition to providing improved model perplexity, this approach significantly \n",
    "            reduces the number of parameters required for the model.\n",
    "        vocab: bpemb entity\n",
    "\n",
    "        \"\"\"\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(vocab.vectors)) # use bpemb embedding\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
    "            self.embedding.weight = self.fc.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)          \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "  \n",
    "    \n",
    "def get_batch(data, seq_len, num_batches, idx):\n",
    "    \"\"\"\n",
    "    given the index of the first batch of tokens in the batch returns the \n",
    "    corresponding batch of sequences.\n",
    "    \n",
    "    data: in [batch size, num_batches] format\n",
    "    seq_len: length of sequence\n",
    "    idx: index\n",
    "\n",
    "    returns: input and targets of the LSTM  \n",
    "    \"\"\"    \n",
    "    src = data[:, idx:idx+seq_len]        # \n",
    "    target = data[:, idx+1:idx+seq_len+1] # next word        \n",
    "    return src, target\n",
    "\n",
    "\n",
    "# Train loop\n",
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \"\"\"\n",
    "    model: LSTM entity\n",
    "    data: result from get_data\n",
    "     \n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    model.train() # training mode - dropout not disabled\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    # The last batch can't be a src\n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  \n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)   \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad(): # we no longer need to backprop or keep track of gradients.\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = 10000\n",
    "num_layers = 2\n",
    "dropout_rate = 0.65  \n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "embedding_dim = 50 \n",
    "hidden_dim = 1024 \n",
    "tie_weights = True if embedding_dim == hidden_dim else False\n",
    "\n",
    "n_epochs = 1\n",
    "seq_len = 5\n",
    "clip = 0.25\n",
    "saved = False\n",
    "\n",
    "bpemb_en = BPEmb(lang=\"en\", dim=embedding_dim, vs=vocab_size) # English (for testing)\n",
    "bpemb_bn = BPEmb(lang=\"bn\", dim=embedding_dim, vs=vocab_size) # Bengali\n",
    "bpemb_ar = BPEmb(lang=\"ar\", dim=embedding_dim, vs=vocab_size) # Arabic\n",
    "bpemb_id = BPEmb(lang=\"id\", dim=embedding_dim, vs=vocab_size) # indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lanuage: arab\n",
      "question_text------------------\n",
      "The model has 23,054,096 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\tTrain Perplexity: 449.084\n",
      "\tValid Perplexity: 193.028\n",
      "Lanuage: arab\n",
      "document_plaintext------------------\n",
      "The model has 23,054,096 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb Cell 17\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m best_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_data, optimizer, criterion, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                 batch_size, seq_len, clip, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, val_data, criterion, batch_size, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                 seq_len, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep(valid_loss)\n",
      "\u001b[1;32m/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(prediction, target)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), clip)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/martin/kode/kurser/NLP/NLP/week_37/lstm.ipynb#Y153sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lang = [(bpemb_ar, train_arab, val_arab, 'arab'), \n",
    "        (bpemb_bn, train_beng, val_beng, 'beng'), \n",
    "        (bpemb_id, train_indo, val_indo, 'indo')]\n",
    "\n",
    "# Train and validate all 6 models\n",
    "for vocab, train_df, val_df, ln in lang:\n",
    "    for doc in ['question_text', 'document_plaintext']:\n",
    "        \n",
    "        print(f'Lanuage: {ln}')\n",
    "        print(f'{doc}------------------')\n",
    "        \n",
    "        train_data = get_data(train_df[doc], vocab, batch_size)\n",
    "        val_data = get_data(val_df[doc], vocab, batch_size)\n",
    "\n",
    "        model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights, vocab=vocab).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        #num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        #print(f'The model has {num_params:,} trainable parameters')\n",
    "\n",
    "        # reduce learning rate as we go along \n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0) \n",
    "\n",
    "        if saved:\n",
    "            model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "            test_loss = evaluate(model, val_data, criterion, batch_size, seq_len, device)\n",
    "            print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
    "        else:\n",
    "            best_valid_loss = float('inf')\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                \n",
    "                train_loss = train(model, train_data, optimizer, criterion, \n",
    "                            batch_size, seq_len, clip, device)\n",
    "                valid_loss = evaluate(model, val_data, criterion, batch_size, \n",
    "                            seq_len, device)\n",
    "                \n",
    "                lr_scheduler.step(valid_loss)\n",
    "\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    name = 'best-val-'+ln+'-'+doc+'.pt'\n",
    "                    torch.save(model.state_dict(), name)\n",
    "\n",
    "                print(f'Epoch: {epoch+1}')\n",
    "                print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "                print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']:\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(prediction)\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "prompt = '' # Some example\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
