{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import Preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Load the copenlu/answerable_tydiqa dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "train_df = train_df[train_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "train_df = p.DataFramePreprocessor(train_df, columns_to_tokenize=['document_plaintext', 'question_text'], remove_stopwords = False).df\n",
    "\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "val_df = val_df[val_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "val_df = p.DataFramePreprocessor(val_df, columns_to_tokenize=['document_plaintext', 'question_text'], remove_stopwords = False).df\n",
    "\n",
    "\n",
    "def compute_normalized_ngram_frequencies(df, column, n, compute_perplexity=True):\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"{column} not found in the DataFrame\")\n",
    "    \n",
    "    ngram_freqs = defaultdict(int)\n",
    "    prefix_counts = defaultdict(int)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        tokens = row[column]\n",
    "        \n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            prefix = tuple(tokens[i:i+n-1])\n",
    "            \n",
    "            ngram_freqs[ngram] += 1\n",
    "            prefix_counts[prefix] += 1\n",
    "    \n",
    "    normalized_ngram_freqs = {}\n",
    "    for ngram, count in ngram_freqs.items():\n",
    "        prefix = ngram[:-1]\n",
    "        normalized_ngram_freqs[ngram] = count / prefix_counts[prefix]\n",
    "    \n",
    "    if not compute_perplexity:\n",
    "        return normalized_ngram_freqs\n",
    "    \n",
    "    def compute_perplexity(df, column, normalized_ngram_freqs, n):\n",
    "        product = 1.0\n",
    "        total_ngrams = 0\n",
    "        \n",
    "        for tokens in df[column]:        \n",
    "            for i in range(n-1, len(tokens)):\n",
    "                ngram = tuple(tokens[i-n+1:i+1])\n",
    "                \n",
    "                # If this n-gram was not in the training data, we skip it\n",
    "                if ngram not in normalized_ngram_freqs:\n",
    "                    continue\n",
    "                \n",
    "                prob = normalized_ngram_freqs[ngram]\n",
    "                product *= (1/prob)\n",
    "                total_ngrams += 1\n",
    "        \n",
    "        # If there were no valid n-grams in the validation set, the perplexity is undefined\n",
    "        if total_ngrams == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Calculate and return the perplexity\n",
    "        return product ** (1/total_ngrams)\n",
    "\n",
    "    # Compute perplexity for validation set\n",
    "    return compute_perplexity(val_df, column, normalized_ngram_freqs, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0037327281712645"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 10, compute_perplexity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 100, compute_perplexity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document perplexity for unigram: 13390.45519933151\n",
      "Document perplexity for bigram: 75.85927459009982\n",
      "Document perplexity for trigram: 4.169447999633632\n",
      "Document perplexity for 4-gram: 1.3455694340722437\n",
      "Document perplexity for 5-gram: 1.0564257088361038\n",
      "Document perplexity for 6-gram: 1.015377084252998\n",
      "Document perplexity for 7-gram: 1.006855356028215\n",
      "Question perplexity for unigram: 346.3580325222089\n",
      "Question perplexity for bigram: 9.621843001657368\n",
      "Question perplexity for trigram: 6.145006896638814\n",
      "Question perplexity for 4-gram: 5.787894352718436\n",
      "Question perplexity for 5-gram: 2.054405685721316\n",
      "Question perplexity for 6-gram: 1.497735987862882\n",
      "Question perplexity for 7-gram: 1.1378808708882513\n"
     ]
    }
   ],
   "source": [
    "print(f\"Document perplexity for unigram: {compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 1)}\")\n",
    "print(f\"Document perplexity for bigram: {compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 2)}\")\n",
    "print(f\"Document perplexity for trigram: {compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 3)}\")\n",
    "print(f\"Document perplexity for 4-gram: {compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 4)}\")\n",
    "print(f\"Document perplexity for 5-gram: {compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 5)}\")\n",
    "print(f\"Document perplexity for 6-gram: {compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 6)}\")\n",
    "print(f\"Document perplexity for 7-gram: {compute_normalized_ngram_frequencies(train_df, 'document_plaintext_tokens', 7)}\")\n",
    "\n",
    "print(f\"Question perplexity for unigram: {compute_normalized_ngram_frequencies(train_df, 'question_text_tokens', 1)}\")\n",
    "print(f\"Question perplexity for bigram: {compute_normalized_ngram_frequencies(train_df, 'question_text_tokens', 2)}\")\n",
    "print(f\"Question perplexity for trigram: {compute_normalized_ngram_frequencies(train_df, 'question_text_tokens', 3)}\")\n",
    "print(f\"Question perplexity for 4-gram: {compute_normalized_ngram_frequencies(train_df, 'question_text_tokens', 4)}\")\n",
    "print(f\"Question perplexity for 5-gram: {compute_normalized_ngram_frequencies(train_df, 'question_text_tokens', 5)}\")\n",
    "print(f\"Question perplexity for 6-gram: {compute_normalized_ngram_frequencies(train_df, 'question_text_tokens', 6)}\")\n",
    "print(f\"Question perplexity for 7-gram: {compute_normalized_ngram_frequencies(train_df, 'question_text_tokens', 7)}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
