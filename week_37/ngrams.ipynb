{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import Preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "# Define a dummy preprocessor since the original is not provided\n",
    "class DataFramePreprocessor:\n",
    "    def __init__(self, df, columns_to_tokenize, remove_stopwords):\n",
    "        self.df = df\n",
    "\n",
    "# Load the copenlu/answerable_tydiqa dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "train_df = train_df[train_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "train_df = DataFramePreprocessor(train_df, columns_to_tokenize=['document_plaintext', 'question_text'], remove_stopwords=False).df\n",
    "\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "val_df = val_df[val_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "val_df = DataFramePreprocessor(val_df, columns_to_tokenize=['document_plaintext', 'question_text'], remove_stopwords=False).df\n",
    "\n",
    "\n",
    "def compute_normalized_ngram_frequencies(df, val_df, column, n, compute_perplexity=True, smoothing=0.0001):\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"{column} not found in the DataFrame\")\n",
    "\n",
    "    ngram_freqs = defaultdict(int)\n",
    "    prefix_counts = defaultdict(int)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        tokens = row[column]\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i + n])\n",
    "            prefix = tuple(tokens[i:i + n - 1])\n",
    "            ngram_freqs[ngram] += 1\n",
    "            prefix_counts[prefix] += 1\n",
    "\n",
    "    vocabulary_size = len(set(word for sentence in df[column] for word in sentence))\n",
    "    normalized_ngram_freqs = {}\n",
    "    for ngram, count in ngram_freqs.items():\n",
    "        prefix = ngram[:-1]\n",
    "        normalized_ngram_freqs[ngram] = (count + smoothing) / (prefix_counts[prefix] + smoothing * vocabulary_size)\n",
    "\n",
    "    if not compute_perplexity:\n",
    "        return normalized_ngram_freqs\n",
    "\n",
    "    def compute_perplexity(df, column, normalized_ngram_freqs, n, vocabulary_size, smoothing):\n",
    "        log_product = 0.0\n",
    "        total_ngrams = 0\n",
    "        for tokens in df[column]:\n",
    "            for i in range(n - 1, len(tokens)):\n",
    "                ngram = tuple(tokens[i - n + 1:i + 1])\n",
    "                prob = normalized_ngram_freqs.get(ngram, smoothing / (smoothing * vocabulary_size))\n",
    "                log_product += -math.log(prob)\n",
    "                total_ngrams += 1\n",
    "        if total_ngrams == 0:\n",
    "            return float('inf')\n",
    "        return math.exp(log_product / total_ngrams)\n",
    "\n",
    "    return compute_perplexity(val_df, column, normalized_ngram_freqs, n, vocabulary_size, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the copenlu/answerable_tydiqa dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "train_df = train_df[train_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "train_df = p.DataFramePreprocessor(train_df, columns_to_tokenize=['document_plaintext', 'question_text'], remove_stopwords=False).df\n",
    "train_arabic = train_df[train_df['language'] == 'arabic']\n",
    "train_indonesian = train_df[train_df['language'] == 'indonesian']\n",
    "train_bengali = train_df[train_df['language'] == 'bengali']\n",
    "\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "val_df = val_df[val_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "val_df = p.DataFramePreprocessor(val_df, columns_to_tokenize=['document_plaintext', 'question_text'], remove_stopwords = False).df\n",
    "val_arabic = val_df[val_df['language'] == 'arabic']\n",
    "val_indonesian = val_df[val_df['language'] == 'indonesian']\n",
    "val_bengali = val_df[val_df['language'] == 'bengali']\n",
    "\n",
    "#print(f\"Document perplexity for bigram arabic: {compute_normalized_ngram_frequencies(df=train_arabic, val_df=val_arabic, column='document_plaintext_tokens', n=2)}\")\n",
    "#print(f\"Document perplexity for bigram indonesian: {compute_normalized_ngram_frequencies(df=train_indonesian, val_df=val_indonesian, column='document_plaintext_tokens', n=2)}\")\n",
    "#print(f\"Document perplexity for bigram bengali: {compute_normalized_ngram_frequencies(df=train_bengali, val_df=val_bengali, column='document_plaintext_tokens', n=2)}\")\n",
    "\n",
    "#print(f\"Question perplexity for bigram arabic: {compute_normalized_ngram_frequencies(df=train_arabic, val_df=val_arabic, column='question_text_tokens', n=2)}\")\n",
    "#print(f\"Question perplexity for bigram indonesian: {compute_normalized_ngram_frequencies(df=train_indonesian, val_df=val_indonesian, column='question_text_tokens', n=2)}\")\n",
    "#print(f\"Question perplexity for bigram bengali: {compute_normalized_ngram_frequencies(df=train_bengali, val_df=val_bengali, column='question_text_tokens', n=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_freqs_docs = compute_normalized_ngram_frequencies(df=train_arabic, val_df=val_arabic, column='document_plaintext_tokens', n=2, compute_perplexity=False)\n",
    "indonesian_freqs_docs = compute_normalized_ngram_frequencies(df=train_indonesian, val_df=val_indonesian, column='document_plaintext_tokens', n=2, compute_perplexity=False)\n",
    "bengali_freqs_docs = compute_normalized_ngram_frequencies(df=train_bengali, val_df=val_bengali, column='document_plaintext_tokens', n=2, compute_perplexity=False)\n",
    "\n",
    "arabic_freqs_questions = compute_normalized_ngram_frequencies(df=train_arabic, val_df=val_arabic, column='question_text_tokens', n=2, compute_perplexity=False)\n",
    "indonesian_freqs_questions = compute_normalized_ngram_frequencies(df=train_indonesian, val_df=val_indonesian, column='question_text_tokens', n=2, compute_perplexity=False)\n",
    "bengali_freqs_questions = compute_normalized_ngram_frequencies(df=train_bengali, val_df=val_bengali, column='question_text_tokens', n=2, compute_perplexity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions\n",
      "[0.19642857142857142, 0.25892857142857145, 0.32142857142857145, 0.42857142857142855, 0.5504587155963303, 0.6116504854368932, 0.6931818181818182, 0.7076923076923077, 0.8222222222222222, 0.7241379310344828]\n",
      "Bengali documents\n",
      "[0.16964285714285715, 0.14285714285714285, 0.15695067264573992, 0.12612612612612611, 0.13122171945701358, 0.15454545454545454, 0.15, 0.14678899082568808, 0.14883720930232558, 0.16822429906542055]\n",
      "Arabic questions\n",
      "[0.6771819137749737, 0.22082018927444794, 0.1808622502628812, 0.2235817575083426, 0.3278955954323002, 0.3958333333333333, 0.4686192468619247, 0.4861111111111111, 0.5116279069767442, 0.4222222222222222]\n",
      "Arabic documents\n",
      "[0.1892744479495268, 0.15615141955835962, 0.15, 0.17223105458399576, 0.16622127204703366, 0.17189189189189188, 0.16111414527580556, 0.16501103752759383, 0.18050139275766017, 0.1858108108108108]\n",
      "Indonesian questions\n",
      "[0.4063811922753988, 0.29303106633081444, 0.23173803526448364, 0.2576419213973799, 0.3706140350877193, 0.45266781411359724, 0.4748201438848921, 0.5316455696202531, 0.65, 0.42857142857142855]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def predict_next_token_accuracy(bigram_freqs, val_df, column, k=1):\n",
    "    accuracies = [0] * 10  # List to store accuracies for each token index (0-9)\n",
    "    total_counts = [0] * 10  # List to store total prediction attempts for each token index (0-9)\n",
    "\n",
    "    # Function to predict next token based on previous token and bigram frequencies\n",
    "    def predict_next_token(prev_token, bigram_freqs, k):\n",
    "        candidates = {bigram[1]: freq for bigram, freq in bigram_freqs.items() if bigram[0] == prev_token}\n",
    "        # Sort candidates by frequency in descending order and take the top k\n",
    "        top_candidates = sorted(candidates, key=candidates.get, reverse=True)[:k]\n",
    "        return top_candidates\n",
    "\n",
    "    # Iterate through each row in the validation dataframe\n",
    "    for _, row in val_df.iterrows():\n",
    "        tokens = row[column]\n",
    "        for i in range(min(10, len(tokens) - 1)):  # Only consider first 10 tokens or less if not available\n",
    "            total_counts[i] += 1\n",
    "            top_candidates = predict_next_token(tokens[i], bigram_freqs, k)\n",
    "            if tokens[i + 1] in top_candidates:\n",
    "                accuracies[i] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    accuracies = [acc / total if total > 0 else 0 for acc, total in zip(accuracies, total_counts)]\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "print(\"Bengali questions\")\n",
    "print(predict_next_token_accuracy(bengali_freqs_questions, val_bengali, 'question_text_tokens', k=3))\n",
    "print(\"Bengali documents\")\n",
    "print(predict_next_token_accuracy(bengali_freqs_docs, val_bengali, 'document_plaintext_tokens', k=3))\n",
    "print(\"Arabic questions\")\n",
    "print(predict_next_token_accuracy(arabic_freqs_questions, val_arabic, 'question_text_tokens', k=3))\n",
    "print(\"Arabic documents\")\n",
    "print(predict_next_token_accuracy(arabic_freqs_docs, val_arabic, 'document_plaintext_tokens', k=3))\n",
    "print(\"Indonesian questions\")\n",
    "print(predict_next_token_accuracy(indonesian_freqs_questions, val_indonesian, 'question_text_tokens', k=3))\n",
    "print(\"Indonesian documents\")\n",
    "print(predict_next_token_accuracy(indonesian_freqs_docs, val_indonesian, 'document_plaintext_tokens', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesian documents\n",
      "[0.1847187237615449, 0.16554621848739495, 0.19208087615838249, 0.1921768707482993, 0.1982832618025751, 0.21872265966754156, 0.1909814323607427, 0.1906474820143885, 0.1912964641885766, 0.1780821917808219]\n"
     ]
    }
   ],
   "source": [
    "print(\"Indonesian documents\")\n",
    "print(predict_next_token_accuracy(indonesian_freqs_docs, val_indonesian, 'document_plaintext_tokens', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ferb', 'bartolomeo', 'ma’juj']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token('dan', indonesian_freqs_questions, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dimanakah letak Donggala ?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_indonesian['question_text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<Q>',\n",
       " 'siapakah',\n",
       " 'karakter',\n",
       " 'utama',\n",
       " 'serial',\n",
       " 'anime',\n",
       " 'dan',\n",
       " 'manga',\n",
       " 'eyeshield',\n",
       " '</Q>',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_indonesian['question_text_tokens'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 224/224 [00:31<00:00,  7.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.044642857142857144,\n",
       " 0.11160714285714286,\n",
       " 0.09821428571428571,\n",
       " 0.08928571428571429,\n",
       " 0.08968609865470852,\n",
       " 0.1036036036036036,\n",
       " 0.12217194570135746,\n",
       " 0.1,\n",
       " 0.08181818181818182,\n",
       " 0.10091743119266056]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def predict_next_token_accuracy(bigram_freqs, val_df, column, k=1):\n",
    "    accuracies = [0] * 10  # List to store accuracies for each token index (0-9)\n",
    "    total_counts = [0] * 10  # List to store total prediction attempts for each token index (0-9)\n",
    "\n",
    "    # Function to predict next token based on previous token and bigram frequencies\n",
    "    def predict_next_token(prev_token, bigram_freqs, k):\n",
    "        candidates = {bigram[1]: freq for bigram, freq in bigram_freqs.items() if bigram[0] == prev_token}\n",
    "        # Sort candidates by frequency in descending order and take the top k\n",
    "        top_candidates = sorted(candidates, key=candidates.get, reverse=True)[:k]\n",
    "        return top_candidates\n",
    "\n",
    "    # Iterate through each row in the validation dataframe\n",
    "    for _, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Processing rows\"):\n",
    "        tokens = row[column]\n",
    "        for i in range(min(10, len(tokens) - 1)):  # Only consider first 10 tokens or less if not available\n",
    "            total_counts[i] += 1\n",
    "            top_candidates = predict_next_token(tokens[i], bigram_freqs, k)\n",
    "            if tokens[i + 1] in top_candidates:\n",
    "                accuracies[i] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    accuracies = [acc / total if total > 0 else 0 for acc, total in zip(accuracies, total_counts)]\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "predict_next_token_accuracy(bengali_freqs_docs, val_bengali, 'document_plaintext_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document perplexity for bigram arabic: 5470.263650772015\n",
      "Document perplexity for bigram indonesian: 2418.98273615176\n",
      "Document perplexity for bigram bengali: 3974.521499262435\n",
      "Question perplexity for bigram arabic: 100.24688224033824\n",
      "Question perplexity for bigram indonesian: 76.26349679305203\n",
      "Question perplexity for bigram bengali: 51.02582039878195\n"
     ]
    }
   ],
   "source": [
    "print(f\"Document perplexity for bigram arabic: {compute_normalized_ngram_frequencies(df=train_arabic, val_df=val_arabic, column='document_plaintext_tokens', n=2)}\")\n",
    "print(f\"Document perplexity for bigram indonesian: {compute_normalized_ngram_frequencies(df=train_indonesian, val_df=val_indonesian, column='document_plaintext_tokens', n=2)}\")\n",
    "print(f\"Document perplexity for bigram bengali: {compute_normalized_ngram_frequencies(df=train_bengali, val_df=val_bengali, column='document_plaintext_tokens', n=2)}\")\n",
    "\n",
    "print(f\"Question perplexity for bigram arabic: {compute_normalized_ngram_frequencies(df=train_arabic, val_df=val_arabic, column='question_text_tokens', n=2)}\")\n",
    "print(f\"Question perplexity for bigram indonesian: {compute_normalized_ngram_frequencies(df=train_indonesian, val_df=val_indonesian, column='question_text_tokens', n=2)}\")\n",
    "print(f\"Question perplexity for bigram bengali: {compute_normalized_ngram_frequencies(df=train_bengali, val_df=val_bengali, column='question_text_tokens', n=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document perplexity for bigram arabic: 5555.463768819053\n",
      "Document perplexity for bigram indonesian: 2553.3478426941274\n",
      "Document perplexity for bigram bengali: 3972.5372565828825\n",
      "Question perplexity for bigram arabic: 100.87734520584422\n",
      "Question perplexity for bigram indonesian: 77.66550552391907\n",
      "Question perplexity for bigram bengali: 51.392352373865464\n"
     ]
    }
   ],
   "source": [
    "print(f\"Document perplexity for bigram arabic: {compute_normalized_ngram_frequencies(df=train_arabic, val_df=val_arabic, column='document_plaintext_tokens', n=2)}\")\n",
    "print(f\"Document perplexity for bigram indonesian: {compute_normalized_ngram_frequencies(df=train_indonesian, val_df=val_indonesian, column='document_plaintext_tokens', n=2)}\")\n",
    "print(f\"Document perplexity for bigram bengali: {compute_normalized_ngram_frequencies(df=train_bengali, val_df=val_bengali, column='document_plaintext_tokens', n=2)}\")\n",
    "\n",
    "print(f\"Question perplexity for bigram arabic: {compute_normalized_ngram_frequencies(df=train_arabic, val_df=val_arabic, column='question_text_tokens', n=2)}\")\n",
    "print(f\"Question perplexity for bigram indonesian: {compute_normalized_ngram_frequencies(df=train_indonesian, val_df=val_indonesian, column='question_text_tokens', n=2)}\")\n",
    "print(f\"Question perplexity for bigram bengali: {compute_normalized_ngram_frequencies(df=train_bengali, val_df=val_bengali, column='question_text_tokens', n=2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
