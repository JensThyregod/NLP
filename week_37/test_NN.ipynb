{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42613b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/oliver/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import Preprocessor as p\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "gen = pipeline('text-generation', model='gpt2')\n",
    "import csv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helpMe import get_ppl, convert_to_float, getList, Average, map_boolean, generate_embeddings, write_embeddings_to_csv\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from customDataset import CustomDataset\n",
    "\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a414d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install scikit-learn\n",
    "\n",
    "#!pip install bltk\n",
    "#!pip install Arabic-Stopwords\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797078d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8876a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the copenlu/answerable_tydiqa dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "train_df = train_df[train_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "train_df = p.DataFramePreprocessor(train_df).df\n",
    "\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "val_df = val_df[val_df['language'].isin(['indonesian', 'arabic', 'bengali'])]\n",
    "val_df = p.DataFramePreprocessor(val_df).df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9d3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df\n",
    "data['document_length'] = data['document_plaintext'].str.len()\n",
    "data['is_answerable'] = data['annotations'].apply(lambda x: x.get('answer_start', [-1])[0] != -1)\n",
    "\n",
    "data.question_text = data.question_text.str[1:].apply('<Q>{}<\\\\Q>'.format)\n",
    "data.document_plaintext = data.document_plaintext.str[1:].apply('<DOC>{}<\\\\DOC>'.format)\n",
    "\n",
    "data['is_answerable'] = data['is_answerable'].apply(map_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ffd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined'] = data.question_text + data.document_plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b405fa7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<Q>ا هي المسألة الشرقية ؟<\\\\Q><DOC>\\nالمسألة الشرقية (بالإنجليزية: Eastern Question) (بالفرنسية: Question de l'orient): هي مسألة وجود العثمانيين المسلمين في أوروبا وطردهم منها واستعادة القسطنطينية من العثمانيين بعد سقوطها في 1453 وتهديد مصالح الدول الأوروبية في هذه المنطقة. كما يدل المصطلح على تصفية أملاك رجل أوروبا المريض في البلقان من طرف الدول الأوروبية.<\\\\DOC>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1]['combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a8ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali = data[data['language'] == 'bengali']\n",
    "arabic = data[data['language'] == 'arabic']\n",
    "indonesian = data[data['language'] == 'indonesian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31304c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e84ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdfe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_val = val_df[val_df['language'] == 'bengali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84aca92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bengali_val['answers'] = bengali_val.apply(extract_answer_text, axis=1)\n",
    "#bengali_val['answers'] = bengali_val['answers'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e3eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3034/2747896391.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indonesian['answers'] = indonesian.apply(extract_answer_text, axis=1)\n",
      "/tmp/ipykernel_3034/2747896391.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  arabic['answers'] = arabic.apply(extract_answer_text, axis=1)\n",
      "/tmp/ipykernel_3034/2747896391.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bengali['answers'] = bengali.apply(extract_answer_text, axis=1)\n",
      "/tmp/ipykernel_3034/2747896391.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bengali['answers'] = bengali['answers'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "def extract_answer_text(row):\n",
    "    answer_text = row['annotations']['answer_text']\n",
    "    return answer_text if answer_text != '' else '</NONE>'\n",
    "\n",
    "\n",
    "# Apply the function to create a new column 'answers'\n",
    "indonesian['answers'] = indonesian.apply(extract_answer_text, axis=1)\n",
    "\n",
    "arabic['answers'] = arabic.apply(extract_answer_text, axis=1)\n",
    "bengali['answers'] = bengali.apply(extract_answer_text, axis=1)\n",
    "bengali['answers'] = bengali['answers'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dde2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def translate_text(text, target_language='en', api_key='AIzaSyD9EpF7MevXh4Z0wv72Hgyw5Mh-IrtNz4s'):\n",
    "    translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "    translation = translator.translate(text, dest=target_language, src='auto', key=api_key)\n",
    "    return translation.text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Translate 'question_text' column from French to English\n",
    "#bengali['english']  = translate_column(bengali, 'be', 'en', 'question_text')\n",
    "#indonesian['english']  = translate_column(indonesian, 'in', 'en', 'question_text')\n",
    "#arabic['english']  = translate_column(arabic, 'ar', 'en', 'question_text')\n",
    "\n",
    "#bengali.to_csv('bengali_with_translate.csv', index=True) \n",
    "#indonesian.to_csv('indonesian_with_translate.csv', index=True) \n",
    "#arabic.to_csv('arabic_with_translate.csv', index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d931cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7b914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16eacabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f0f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13f3f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenizer(bengali['question_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "small_eval_dataset = tokenizer(bengali_val['question_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9b3530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(output_dir=\"gpt2_beng\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5282dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c22bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4900421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Tokenize the text data from the DataFrame\n",
    "question_tokens = tokenizer(bengali['question_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "answer_tokens = tokenizer(bengali['answers'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Concatenate the tokenized sequences\n",
    "input_ids = torch.cat([question_tokens.input_ids, answer_tokens.input_ids], dim=1)\n",
    "attention_mask = torch.cat([question_tokens.attention_mask, answer_tokens.attention_mask], dim=1)\n",
    "\n",
    "def custom_data_collator(features):\n",
    "    # Print the shapes before squeezing\n",
    "    for feature in features:\n",
    "        print(feature[0].shape)  # Print the shape of input_ids tensor before squeezing\n",
    "        print(feature[1].shape)  # Print the shape of attention_mask tensor before squeezing\n",
    "    \n",
    "    # Squeeze the tensors to remove the extra dimension\n",
    "    input_ids = torch.cat([torch.squeeze(feature[0]) for feature in features], dim=0)\n",
    "    attention_mask = torch.cat([torch.squeeze(feature[1]) for feature in features], dim=0)\n",
    "    \n",
    "    # Print the shapes after squeezing\n",
    "    print(input_ids.shape)  # Print the shape of input_ids tensor after squeezing\n",
    "    print(attention_mask.shape)  # Print the shape of attention_mask tensor after squeezing\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "# Create a Trainer instance with the custom data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir='./output',  # Directory where the fine-tuned model will be saved\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,  # Number of training epochs\n",
    "        per_device_train_batch_size=4,  # Batch size per GPU\n",
    "        save_steps=10_000,  # Save the model checkpoint every X steps\n",
    "        save_total_limit=2,  # Limit the total number of saved checkpoints\n",
    "    ),\n",
    "    train_dataset=torch.utils.data.TensorDataset(input_ids, attention_mask),\n",
    "    data_collator=custom_data_collator  # Use the custom data collator\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_gpt2_model')\n",
    "tokenizer.save_pretrained('fine_tuned_gpt2_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23acb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188b09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4001c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppl_ara = getList('arabic.csv')[0]\n",
    "#ppl_ben = getList('bengali.csv')[0]\n",
    "#ppl_ind = getList('indonesian.csv')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19538dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = Average(convert_to_float(ppl_ara))\n",
    "be = Average(convert_to_float(ppl_ben))\n",
    "ind = Average(convert_to_float(ppl_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df9476a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic: 20.281176299731527\n",
      "Bengali: 5.924668006843454\n",
      "Indonesian : 3924.599611058437\n"
     ]
    }
   ],
   "source": [
    "print(f'Arabic: {ar}')\n",
    "print(f'Bengali: {be}')\n",
    "print(f'Indonesian : {ind}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e15c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e473432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfd36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388d2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Function to generate embeddings for text\n",
    "def generate_embeddings(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=512)  # Adjust max_length as per your requirements\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling for sentence embeddings\n",
    "    return embeddings.numpy()\n",
    "\n",
    "texts = indonesian['combined']\n",
    "\n",
    "csv_file = 'embeddings.csv'\n",
    "\n",
    "batch_size = 1\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    embeddings_list = []\n",
    "    for text in batch_texts:\n",
    "        embeddings = generate_embeddings(text)\n",
    "        embeddings_list.append(embeddings)\n",
    "    df = pd.DataFrame({'text': batch_texts, 'embeddings': embeddings_list})\n",
    "    df.to_csv(csv_file, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b9dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb39149",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = 'embeddings_bengali.csv'\n",
    "bengali_embeddings = pd.read_csv(emb,header=None)\n",
    "bengali_embeddings = bengali_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b4d4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.array(bengali['is_answerable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40a41f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Function to convert string representation of NumPy array to NumPy array\n",
    "def convert_to_numpy_array(s):\n",
    "    # Extract numeric values from the string using regular expression\n",
    "    numeric_values = re.findall(r'-?\\d+\\.\\d+', s)\n",
    "    # Convert the numeric values to float and create a NumPy array\n",
    "    numpy_array = np.array(numeric_values, dtype=float)\n",
    "    # Reshape the NumPy array based on the original shape of the array\n",
    "    # Replace (3, 4) with the original shape of your array\n",
    "   \n",
    "    return numpy_array\n",
    "\n",
    "# Apply the function to the Pandas Series to get NumPy arrays\n",
    "bengali_array = bengali_embeddings.apply(convert_to_numpy_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fbf2ba7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bengali_array[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80d953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_data, targets):\n",
    "        self.input_data = input_data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_sample = self.input_data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return input_sample, target\n",
    "\n",
    "# Assuming input_data and targets are PyTorch tensors of shape (num_samples, 768) and (num_samples,) respectively\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = CustomDataset(bengali_array, ys)\n",
    "\n",
    "# DataLoader for training data\n",
    "batch_size = 32  # You can adjust the batch size based on your preference and available memory\n",
    "train_loader = DataLoader(dataset=custom_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLinearNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleLinearNN, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: compute predicted y by passing x to the model\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# Assuming input_size is 768 and output_size is 1 for binary classification\n",
    "input_size = 768\n",
    "output_size = 1\n",
    "\n",
    "# Create the model\n",
    "model = SimpleLinearNN(input_size, output_size)\n",
    "\n",
    "# Ensure that the model and input tensors have the same data type (e.g., float32)\n",
    "model = model.float()  # Set the model data type to float32\n",
    "\n",
    "# Convert the input tensors to the appropriate data type (float32)\n",
    "inputs = inputs.float()  # Assuming inputs is your input tensor\n",
    "# Assuming you have your data loaded into DataLoader train_loader and processed.\n",
    "# Define the binary cross-entropy loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent in this case)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets.float())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "# After training, you can use the model for predictions\n",
    "# For example:\n",
    "# test_outputs = model(test_inputs)\n",
    "# predicted_labels = torch.round(torch.sigmoid(test_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85efd703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ad9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5574d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdd1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb56b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ca87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143756b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5aa5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d64567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1dfe62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070a8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b156e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ced25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d2a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbedab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f403037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
