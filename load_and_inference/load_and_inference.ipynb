{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece ## restart kernel efter install hvis du mangler denne ##\n",
    "!pip install accelerate -U\n",
    "!pip install numpy --upgrade\n",
    "!pip install tensorflow --upgrade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "arabic_model_path = \"arabic_xlm_roberta_qa_model\"\n",
    "indonesian_model_path = \"/Users/jensthyregod/Desktop/KU/7. Semester/NLP/load_and_inference/indo_xlm_roberta_qa_model\"\n",
    "bengali_model_path = \"/Users/jensthyregod/Desktop/KU/7. Semester/NLP/load_and_inference/bengali_xlm_roberta_qa_model\"\n",
    "\n",
    "def load_transformer_model(model_path):\n",
    "    \"\"\"\n",
    "    Load a transformer model and tokenizer from a given directory.\n",
    "\n",
    "    :param model_path: Path to the directory containing model files.\n",
    "    :return: A tuple of (model, tokenizer).\n",
    "    \"\"\"\n",
    "    # Load pre-trained model\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "    \n",
    "    # Load pre-trained model tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "arabic_model, arabic_tokenizer = load_transformer_model(arabic_model_path)\n",
    "indonesian_model, indonesian_tokenizer = load_transformer_model(indonesian_model_path)\n",
    "bengali_model, bengali_tokenizer = load_transformer_model(bengali_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load datas√¶ttet tokenize og preprocesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def preprocess_tydiqa_dataset(language, tokenizer, dataset_subset=1.0):\n",
    "    # Load the dataset\n",
    "    tydiqa_dataset = load_dataset('copenlu/answerable_tydiqa')\n",
    "\n",
    "    # Filter the dataset for the specified language\n",
    "    train_dataset = tydiqa_dataset[\"train\"].filter(lambda example: example['language'] == language)\n",
    "    val_dataset = tydiqa_dataset[\"validation\"].filter(lambda example: example['language'] == language)\n",
    "\n",
    "    # Sample a subset of the dataset\n",
    "    train_dataset = train_dataset.shuffle(seed=42).select(range(int(len(train_dataset) * dataset_subset)))\n",
    "    val_dataset = val_dataset.shuffle(seed=42).select(range(int(len(val_dataset) * dataset_subset)))\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the examples\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples['question_text'],\n",
    "            examples['document_plaintext'],\n",
    "            truncation=\"only_second\",\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "\n",
    "        # Extract overflow_to_sample_mapping and remove it from tokenized_inputs\n",
    "        overflow_to_sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
    "        offset_mappings = tokenized_inputs.pop(\"offset_mapping\")\n",
    "\n",
    "        # Initialize new lists for storing outputs\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        answer_texts = []\n",
    "        \n",
    "        # Iterate through the annotations and calculate start and end token positions\n",
    "        for i, offsets in enumerate(offset_mappings):\n",
    "            parent_id = overflow_to_sample_mapping[i]\n",
    "            answer_start = examples['annotations'][parent_id]['answer_start'][0]\n",
    "            answer_text = examples['annotations'][parent_id]['answer_text'][0]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            # Find the start and end token index for the answer\n",
    "            start_token_idx = end_token_idx = 0\n",
    "            for idx, (start, end) in enumerate(offsets):\n",
    "                if start <= answer_start < end:\n",
    "                    start_token_idx = idx\n",
    "                if start < answer_end <= end:\n",
    "                    end_token_idx = idx\n",
    "                    break\n",
    "\n",
    "            start_positions.append(start_token_idx)\n",
    "            end_positions.append(end_token_idx)\n",
    "            answer_texts.append(answer_text)\n",
    "\n",
    "        # Return the new lists as a dictionary\n",
    "        return {\n",
    "            'input_ids': tokenized_inputs['input_ids'],\n",
    "            'attention_mask': tokenized_inputs['attention_mask'],\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions,\n",
    "            'answer_texts': answer_texts\n",
    "        }\n",
    "\n",
    "    # Preprocess the datasets\n",
    "    train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "    val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "language = 'bengali'\n",
    "train_dataset, val_dataset = preprocess_tydiqa_dataset(language, bengali_tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'answer_texts'],\n",
       "    num_rows: 231\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=tensor(2.0060), start_logits=tensor([[  1.6130,  -1.0502,   1.5515,  ..., -10.2602, -10.2602, -10.2602],\n",
      "        [  2.1556,  -0.8775,  -3.9784,  ..., -10.1693, -10.1693, -10.1693],\n",
      "        [  2.8227,  -4.2698,  -7.3443,  ..., -10.1205, -10.1205, -10.1205],\n",
      "        ...,\n",
      "        [  1.8547,   3.0382,   2.7313,  ..., -10.1752, -10.1752, -10.1752],\n",
      "        [  2.8709,  -2.9245,  -5.7329,  ..., -10.1765, -10.1765, -10.1765],\n",
      "        [  1.4706,  -2.7841,   0.1186,  ..., -10.2539, -10.2539, -10.2539]]), end_logits=tensor([[  1.4453,  -2.7030,  -0.5691,  ..., -10.1299, -10.1299, -10.1299],\n",
      "        [  2.0750,  -4.4722,  -4.5973,  ..., -10.2238, -10.2238, -10.2238],\n",
      "        [  2.7867,  -5.3089,  -6.2024,  ..., -10.2346, -10.2346, -10.2346],\n",
      "        ...,\n",
      "        [  1.6457,   1.3429,   2.1242,  ..., -10.2027, -10.2027, -10.2027],\n",
      "        [  2.7924,  -4.2957,  -5.5351,  ..., -10.1938, -10.1938, -10.1938],\n",
      "        [  1.3832,  -3.9120,  -2.7065,  ..., -10.0848, -10.0848, -10.0848]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "# Move model to CPU\n",
    "bengali_model = bengali_model.to(\"cpu\")\n",
    "\n",
    "# Collate function to prepare data batches\n",
    "data_collator = DefaultDataCollator(return_tensors=\"pt\")\n",
    "\n",
    "# DataLoader for validation set\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Evaluation loop\n",
    "bengali_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        # Move batch to CPU\n",
    "        batch = {k: v.to(\"cpu\") for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = bengali_model(**batch)\n",
    "        print(outputs)\n",
    "        break\n",
    "        # evaluer her\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.4453,  -2.7030,  -0.5691,  -1.2203,  -0.2837,  -0.3705,  -1.3340,\n",
       "         -1.2962,  -1.5814,  -0.6314,  -0.9357,   0.1690,   0.1076,   0.0597,\n",
       "         -0.3225,  -1.0560,  -8.6460,  -9.6692, -10.1374, -10.5395,  -9.9485,\n",
       "         -8.7089,  -8.5297,  -9.4603,  -9.9376,  -9.5110,  -8.7125,  -8.9490,\n",
       "         -9.8542,  -8.7693,  -9.4770,  -6.4012,  -5.8331,  -8.8367,  -8.2149,\n",
       "         -8.3723,  -5.7131,  -8.8147,  -5.9789,  -5.5913,  -0.5243,  -3.3159,\n",
       "         -8.4252,  -7.3579,  -9.3981, -10.4337, -10.2074,  -9.5572,  -7.7554,\n",
       "         -8.0769,  -4.5249,  -9.1921,  -9.6442,  -9.6898,  -9.4602,  -9.4747,\n",
       "         -9.1562,  -9.2654,  -5.7047,  -6.9601,  -2.8232,   2.1845,   2.5153,\n",
       "          5.4442,   9.3694,   4.7998,   8.2264,  -7.8084,  -7.0464,  -2.2540,\n",
       "         -7.8890,  -7.9291,  -8.3884,  -7.5835,  -5.5980,  -8.7189,  -8.2548,\n",
       "         -4.3544,  -7.9930,  -9.1902,  -6.3661,  -7.4894,  -6.7069,   0.9392,\n",
       "         -1.0145,  -7.7855,  -6.4676,  -7.3158,  -5.9406,   0.9542,  -5.1996,\n",
       "         -8.8934,  -6.5338,  -6.8396,  -6.1726,   1.2109,  -4.2958,  -6.4557,\n",
       "         -7.7760,  -7.4428,   0.6206,  -8.5940,  -9.5124,  -9.7118, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299, -10.1299,\n",
       "        -10.1299])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
